Kafka Fundamentals (Experienced-Level)

1. Why did your organization choose Kafka over traditional messaging systems?

What problems does Kafka solve well—and what does it not solve?

How does Kafka guarantee message ordering?

What is the role of partitions in Kafka scalability?

How do producers decide which partition a message goes to?

What happens if a partition leader fails?

How does Kafka ensure durability?

What are ISR (In-Sync Replicas)?

What happens when ISR shrinks?

How does Kafka handle backpressure?

Producers (Deep Dive)

1.  What delivery guarantees does Kafka provide?
    Kafka supports at-most-once, at-least-once, and exactly-once delivery semantics, 
    depending on producer, consumer, and transaction configuration.
        At-most-once: Messages are processed at most once and may be lost but never duplicated.
        At-least-once: Messages are never lost but may be processed more than once.
        Exactly-once: Messages are processed once and only once, with no loss or duplication.

2.  How do acks=0, acks=1, and acks=all differ in practice?
    acks=0: Producer does not wait for any broker response → fastest, but messages can be lost silently.
    acks=1: Producer waits for leader acknowledgment only → good balance, but data can be lost if the 
            leader fails before replication.
    acks=all: Producer waits for all in-sync replicas to acknowledge → strongest durability, slower throughput.

3.  When can Kafka acknowledge a message but still lose it?
    Kafka can acknowledge a message but still lose it when acks=1 and the leader fails before the 
    message is replicated to in-sync replicas.

4.  How do retries work in Kafka producers?
    When a producer sends a message and doesn’t receive an acknowledgment 
    (due to a transient error like leader change or network issue), 
    Kafka automatically retries sending the same message.
        The producer retries up to retries times
        Retries are spaced by retry.backoff.ms
        Retries happen only for retriable errors (not for invalid messages)

    Important side effect
    If retries are enabled without idempotence, the same message can be written multiple times, 
    causing duplicates.

    Best practice
    Enable idempotent producer (enable.idempotence=true)
    Use acks=all
    This ensures retries do not create duplicates.


5.  How do you avoid message duplication on producer retries?
    To avoid duplication when producer retries happen in Kafka:
    - Enable idempotent producer (enable.idempotence=true) so Kafka deduplicates retries using 
      producer ID + sequence numbers.
    - Use acks=all (and keep a proper min.insync.replicas) so acks mean the record is safely replicated.
    - If you need exactly-once across topics/processing, use transactions (transactional.id) 
      with read_committed consumers.

6.  What is idempotent producer and when do you enable it?
    An idempotent producer ensures that even if a producer retries sending a message, 
    Kafka writes it only once—no duplicates.
    - How it works: Kafka assigns a producer ID and sequence numbers to records; 
      the broker detects and drops duplicate retries.

    - When to enable it:
      Enable it whenever reliability matters and retries are possible, especially with acks=all, 
      leader failovers, or unstable networks—common in production systems.

7.  How does batching improve producer throughput?
    Batching lets a Kafka producer accumulate multiple records and send them together in a single 
    request instead of sending each message individually. It is reducing network and broker overhead.

8.  What are common causes of producer latency?
    High acks setting (especially acks=all) → producer waits for replica confirmations
    Large batch wait time (linger.ms) → producer delays send to fill batches
    Broker load or slow disk I/O → brokers take longer to write and acknowledge
    Network latency / throttling → slow producer–broker communication
    ISR issues (slow or shrinking replicas) → delays acknowledgments
    Retries due to transient errors → resend adds extra time
    Large message size → more time to serialize, transfer, and write

9.  How do you tune producer performance?
    To tune Kafka producer performance, focus on throughput vs latency trade-offs:
    - Throughput (maximize)
        Increase batch.size (bigger batches)
        Increase linger.ms (wait a bit to form batches)
        Use compression (compression.type=lz4 or snappy)
        Set acks=1 if you can accept small durability risk
        Increase buffer.memory if you produce fast

    - Reliability (safe + still fast)
        Use acks=all
        Enable idempotence (enable.idempotence=true)
        Set reasonable retries and delivery.timeout.ms
    
    - Latency (minimize)
        Lower linger.ms
        Reduce batch.size
        Avoid heavy compression if CPU bound

10. What producer misconfigurations have you seen in production?
    Common Kafka producer misconfigurations I’ve seen in production:
        acks=0 used unknowingly → silent data loss
        Retries enabled but idempotence disabled → duplicate messages
        acks=all with min.insync.replicas=1 → weaker durability than expected
        Too small batch.size / linger.ms=0 → poor throughput, high CPU/network overhead
        No compression (or wrong compression) → high bandwidth + broker load
        Message too large vs max.request.size / broker limits → send failures/timeouts
        Bad timeout settings (request.timeout.ms, delivery.timeout.ms) → frequent timeouts/retries
        Too little buffer.memory → producer blocks (buffer exhausted) under load
        Wrong partitioning (hot partition) → high latency on one partition, uneven load
        Not handling backpressure → app keeps producing while broker is slow, causing retries and spikes

        1) Messages missing (no errors)

        Symptom: Downstream says “events are missing,” producer logs look fine
        Root cause: acks=0 (fire-and-forget)
        Fix: Use acks=all (or at least acks=1) + monitoring for send errors

        2) Duplicate events in consumer/DB

        Symptom: Same order/trade/event appears twice
        Root cause: Retries enabled but idempotence off → resend creates duplicates
        Fix: enable.idempotence=true; consider transactions if EOS needed

        3) “We set acks=all but still lost data”

        Symptom: Producer got acks, but records disappeared after broker failure
        Root cause: acks=all but topic has min.insync.replicas=1 (weak durability)
        Fix: Set min.insync.replicas>=2 and keep replication factor ≥ 3

        4) High producer latency spikes

        Symptom: P99 send latency jumps, app slows
        Root cause: Too much waiting for batches (linger.ms high) or broker/ISR slow with acks=all
        Fix: Reduce linger.ms if latency-sensitive; scale brokers/partitions; ensure ISR healthy

        5) Low throughput, high CPU, many small requests

        Symptom: Producer CPU high, network packets high, throughput low
        Root cause: batch.size too small and/or linger.ms=0 → no batching
        Fix: Increase batch.size, set small linger.ms (e.g., 5–20ms), enable compression

        6) High bandwidth cost / broker CPU high

        Symptom: Network saturated, brokers CPU high
        Root cause: Compression disabled or inefficient choice
        Fix: Enable compression.type=lz4 (common balance) or zstd if CPU allows

        7) “RecordTooLarge” / send failures for some messages

        Symptom: Large events fail; small ones work
        Root cause: max.request.size (producer) / message.max.bytes (broker/topic) mismatch
        Fix: Align limits; avoid huge messages (store payload in S3, send pointer)

        8) Frequent timeouts and retry storms

        Symptom: TimeoutException, retries explode, latency worse
        Root cause: Too low request.timeout.ms / delivery.timeout.ms, or unstable broker/network
        Fix: Set sane timeouts; tune retries; fix broker/network bottleneck; keep idempotence on

        9) Producer “blocks” or “buffer exhausted”

        Symptom: App threads hang on send; “buffer memory exhausted”
        Root cause: buffer.memory too small or broker too slow → backpressure
        Fix: Increase buffer.memory; tune batching/compression; scale brokers; apply app-level backpressure

        10) One partition overloaded (hot partition)

        Symptom: Some consumers lag; one partition has huge traffic
        Root cause: Bad keying/partitioner (e.g., constant key), too few partitions
        Fix: Improve key distribution; add partitions; use sticky partitioning for keyless events


Consumers (Deep Dive)

1. How does Kafka consumer group coordination work?
    A consumer group allows multiple consumers to share the work of reading from a topic. 
    Kafka ensures that each partition is consumed by only one consumer within the group.
    When consumers start, they register with a Group Coordinator (a broker chosen by Kafka). 
    One consumer becomes the group leader. The leader gathers metadata (partitions, members) and 
    decides how partitions should be assigned (using a partition assignment strategy). 
    This assignment is then sent to all consumers.
    If a consumer joins, leaves, crashes, or times out, Kafka triggers a rebalance.
     During a rebalance, partitions are revoked and reassigned so the group stays consistent. 
     Consumers then resume reading from their last committed offsets.
    Heartbeats from consumers keep them active in the group; if heartbeats stop, 
    Kafka assumes failure and reassigns partitions.

2. What causes consumer rebalancing?
    Consumer rebalancing in Kafka happens when the membership or assignment of a consumer group changes.
    Common causes include a consumer joining or leaving the group, a consumer failure or missed heartbeats, topic partition changes, or configuration changes (like assignment strategy).
    When this happens, Kafka pauses consumption and redistributes partitions among active consumers.

3. Why are frequent rebalances dangerous?
    Frequent rebalances are dangerous because:
     - Consumption stops during rebalancing, directly impacting throughput
     - They increase consumer lag and end-to-end latency
     - They can cause duplicate processing or missed commits
     - They indicate an unstable consumer group, often due to bad configs or slow consumers

4. What is the role of poll() in Kafka consumers?
    - poll() is the core method that drives a Kafka consumer.
    - It fetches records from the broker and sends heartbeats to the group coordinator to 
            keep the consumer alive.
    - If poll() is not called within the configured interval, Kafka assumes the consumer 
        is dead and triggers a rebalance.

5. What happens if poll() is delayed too long?
    If poll() is delayed too long:
        The consumer stops sending heartbeats
        The group coordinator marks it as dead
        A rebalance is triggered
        Its partitions are revoked and reassigned to other consumers
        The slow consumer may later rejoin, causing more rebalances and possible duplicates

6. How does max.poll.interval.ms affect consumers?
    - max.poll.interval.ms defines the maximum time a consumer can take to process records 
      between two poll() calls.
    - If processing takes longer than this value, Kafka assumes the consumer is stuck, 
      removes it from the group, and triggers a rebalance.

7. Difference between auto-commit and manual commit?
    - Auto-commit
        Kafka commits offsets automatically at intervals
        Simple to use, but can cause message loss or duplicates
        Less control over when a message is considered processed

    - Manual commit
        Application commits offsets explicitly after processing
        Gives full control and reliability
        Slightly more code, but safer for production

8. When can offsets be committed but processing fails?
    Offsets can be committed but processing can still fail in these cases:
    - Auto-commit enabled: Offsets are committed before processing completes, and the app crashes 
       during processing.
    - Manual commit done too early: The app commits offsets before writing to DB / downstream system, 
       then fails.
    - Async commit (commitAsync): Commit succeeds, but processing fails immediately after.
    - External system failure: Kafka offset is committed, but DB/API update fails.

9. How do you design idempotent consumers?
    To design idempotent consumers, make your processing safe to run more than once for the same message (because Kafka is often at-least-once).

    Common patterns (used in production)
    1) Use a unique event id + dedup store
        Include eventId (UUID/orderId+version) in each message
        Before processing, check a table/cache: “have I processed this eventId?”
        If yes → skip; if no → process and record it

    2) Make the downstream write idempotent
        Use UPSERT / INSERT … ON CONFLICT DO NOTHING
        Use unique constraints on eventId or (businessKey, version)
        Avoid “add 10” updates; prefer “set state to X” updates

    3) Transactional pattern (best)
        In one DB transaction:
        apply business update
        store eventId (or update a version)
        Commit Kafka offset after DB commit

    4) Use exactly-once tools when applicable
        Kafka Streams EOS / transactions can help for Kafka→Kafka pipelines, but DB still needs idempotency.

10.What consumer bugs have caused production incidents?
    Here are real consumer bugs that commonly cause production incidents, phrased the way interviewers like (symptom + root cause).
    - Rebalance storm: poll() blocked by long processing → missed heartbeats / max.poll.interval.ms exceeded → constant rebalances, no progress.
    - Duplicate processing: manual commit done after processing but downstream isn’t idempotent → retries/rebalances reprocess messages and create duplicates.
    - Message loss: offsets committed too early (auto-commit or wrong manual commit timing) → crash after commit → records skipped forever.
    - Hot partition lag: bad keying (same key) → one partition overloaded → one consumer maxed out, group looks “healthy” but latency spikes.
    - Infinite retry loop: poison message keeps failing and is retried forever → partition stuck, backlog grows.
    - OOM / GC pauses: fetching huge batches or large messages → JVM GC stalls → missed heartbeats → rebalances + lag.
    - Threading mistake: using one KafkaConsumer across multiple threads → unstable behavior, missed polls, exceptions, stalled consumption.
    - Wrong offset reset: misused auto.offset.reset (e.g., latest in a new group) → starts after existing data → “missing events” incident.
    - Bad error handling: swallowing exceptions and continuing commits → silently drops failed records.
    - Under-provisioned consumers: too few consumers vs partitions or too slow processing → lag grows until downstream SLAs break.


Offset Management & Delivery Semantics

What does “at-least-once” delivery really mean?

Why is “exactly-once” often misunderstood in Kafka?

How does Kafka implement exactly-once semantics?

When is exactly-once not worth the complexity?

What happens if offsets are committed before DB writes?

How do you safely replay Kafka messages?

How do you handle duplicate events?

How do you reset offsets in production safely?

How do you reprocess historical data?

What offset-related outages have you seen?

Topic & Partition Design

How do you decide the number of partitions?

What happens if you under-partition a topic?

Can you change partition count later? What are the risks?

How does partition count affect consumer scaling?

How do keys impact data distribution?

What happens with hot partitions?

How do you design topics for multi-tenant systems?

Topic-per-event vs shared-topic — trade-offs?

How do you handle topic explosion?

What topic design mistakes have you fixed?

Kafka Storage & Retention

How does Kafka store messages on disk?

What is log compaction?

When would you use log compaction over retention?

How does Kafka clean up old data?

What happens when disks fill up?

How do you size Kafka storage?

How does retention impact performance?

How do you handle large messages?

Why is Kafka not a database?

What data-loss scenarios exist in Kafka?

Schema & Data Evolution

How do you manage schema evolution in Kafka?

What happens when producers and consumers use different schemas?

How do you handle backward and forward compatibility?

Schema Registry — why and when?

What schema mistakes cause outages?

How do you version events safely?

How do you deprecate fields in events?

What happens if schema compatibility rules are ignored?

How do you test schema changes?

What real schema evolution failures have you handled?

Error Handling & Reliability

How do you handle poison messages?

Retry topics vs dead-letter topics — when to use what?

How do you avoid infinite retry loops?

How do you detect stuck partitions?

What happens when a consumer crashes mid-processing?

How do you monitor consumer lag correctly?

How do you design failure isolation?

How do you prevent cascading failures via Kafka?

How do you pause and resume consumption safely?

What reliability trade-offs have you made?

Performance & Tuning

What metrics matter most for Kafka performance?

How do you tune brokers for high throughput?

How do you tune consumers for low latency?

How do you detect network-related Kafka issues?

How does compression affect performance?

How do you prevent GC issues in Kafka clients?

What causes throughput to drop suddenly?

How do you benchmark Kafka?

What tuning changes caused outages?

What performance bottlenecks are hardest to diagnose?

Kafka in Distributed Systems

How do you design event-driven systems with Kafka?

Kafka vs REST — when to use which?

How do you guarantee ordering across services?

How do you handle eventual consistency?

How do you manage inter-service contracts?

How do you design workflows using Kafka?

What anti-patterns exist in event-driven design?

How do you control blast radius with Kafka?

How do you handle cross-region Kafka?

What distributed system failures involved Kafka?

Kafka Operations & Production

How do you upgrade Kafka safely?

What breaks during Kafka upgrades?

How do you add brokers to a live cluster?

How do you rebalance partitions?

How do you detect broker failures?

How do you handle disk failures?

How do you secure Kafka in production?

How do you manage ACLs?

What Kafka incidents have you personally handled?

What Kafka decision do you regret?

Senior / Architect-Level Questions

When should Kafka NOT be used?

How do you design Kafka for 100+ services?

How do you prevent Kafka becoming a bottleneck?

How do you enforce governance around Kafka usage?

How do you handle multi-team ownership of topics?

How do you migrate from MQ to Kafka?

How do you justify Kafka operational cost?

How do you design DR for Kafka?

What would you redesign in your Kafka setup?

What Kafka failure taught you the most?