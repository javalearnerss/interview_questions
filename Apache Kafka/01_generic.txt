Kafka Fundamentals (Experienced-Level)

1. Why did your organization choose Kafka over traditional messaging systems?

2. What problems does Kafka solve well‚Äîand what does it not solve?

3. How does Kafka guarantee message ordering?

4. What is the role of partitions in Kafka scalability?

5. How do producers decide which partition a message goes to?

6. What happens if a partition leader fails?

7. How does Kafka ensure durability?

8. What are ISR (In-Sync Replicas)?

9. What happens when ISR shrinks?

10. How does Kafka handle backpressure?

Producers (Deep Dive)

0. When are Producer ID (PID) and sequence numbers assigned?
    Producer ID (PID)
        Assigned once, when the producer starts and idempotence is enabled
        Happens on the first request to the broker (InitProducerId request)
        Broker returns a unique PID for that producer instance
    Sequence numbers
        Assigned by the producer client, not the broker
        Start at 0 per (PID + topic-partition)
        Incremented for every record sent to that partition
    Flow (simple):
        Producer starts with enable.idempotence=true
        Producer contacts broker ‚Üí broker assigns PID
        Producer sends records:
        Each record gets a sequence number per partition
        Broker tracks (PID, partition, sequence)
        If a retry arrives with the same sequence number ‚Üí broker drops it as duplicate

1.  What delivery guarantees does Kafka provide?
    Kafka supports at-most-once, at-least-once, and exactly-once delivery semantics, 
    depending on producer, consumer, and transaction configuration.
        At-most-once: Messages are processed at most once and may be lost but never duplicated.
        At-least-once: Messages are never lost but may be processed more than once.
        Exactly-once: Messages are processed once and only once, with no loss or duplication.

2.  How do acks=0, acks=1, and acks=all differ in practice?
    acks=0: Producer does not wait for any broker response ‚Üí fastest, but messages can be lost silently.
    acks=1: Producer waits for leader acknowledgment only ‚Üí good balance, but data can be lost if the 
            leader fails before replication.
    acks=all: Producer waits for all in-sync replicas to acknowledge ‚Üí strongest durability, slower throughput.

3.  When can Kafka acknowledge a message but still lose it?
    Kafka can acknowledge a message but still lose it when acks=1 and the leader fails before the 
    message is replicated to in-sync replicas.

4.  How do retries work in Kafka producers?
    When a producer sends a message and doesn‚Äôt receive an acknowledgment 
    (due to a transient error like leader change or network issue), 
    Kafka automatically retries sending the same message.
        The producer retries up to retries times
        Retries are spaced by retry.backoff.ms
        Retries happen only for retriable errors (not for invalid messages)

    Important side effect
    If retries are enabled without idempotence, the same message can be written multiple times, 
    causing duplicates.

    Best practice
    Enable idempotent producer (enable.idempotence=true)
    Use acks=all
    This ensures retries do not create duplicates.


5.  How do you avoid message duplication on producer retries?
    To avoid duplication when producer retries happen in Kafka:
    - Enable idempotent producer (enable.idempotence=true) so Kafka deduplicates retries using 
      producer ID + sequence numbers.
    - Use acks=all (and keep a proper min.insync.replicas) so acks mean the record is safely replicated.
    - If you need exactly-once across topics/processing, use transactions (transactional.id) 
      with read_committed consumers.

6.  What is idempotent producer and when do you enable it?
    An idempotent producer ensures that even if a producer retries sending a message, 
    Kafka writes it only once‚Äîno duplicates.
    - How it works: Kafka assigns a producer ID and sequence numbers to records; 
      the broker detects and drops duplicate retries.

    - When to enable it:
      Enable it whenever reliability matters and retries are possible, especially with acks=all, 
      leader failovers, or unstable networks‚Äîcommon in production systems.

7.  How does batching improve producer throughput?
    Batching lets a Kafka producer accumulate multiple records and send them together in a single 
    request instead of sending each message individually. It is reducing network and broker overhead.

8.  What are common causes of producer latency?
    Latency = the time taken for a request to go from sender ‚Üí processing ‚Üí response.
        High acks setting (especially acks=all) ‚Üí producer waits for replica confirmations
        Large batch wait time (linger.ms) ‚Üí producer delays send to fill batches
        Broker load or slow disk I/O ‚Üí brokers take longer to write and acknowledge
        Network latency / throttling ‚Üí slow producer‚Äìbroker communication
        ISR issues (slow or shrinking replicas) ‚Üí delays acknowledgments
        Retries due to transient errors ‚Üí resend adds extra time
        Large message size ‚Üí more time to serialize, transfer, and write

9.  How do you tune producer performance?
    Latency is how long something takes; throughput is how much you can do per unit time.
    To tune Kafka producer performance, focus on throughput vs latency trade-offs:
    - Throughput (maximize)
        Increase batch.size (bigger batches)
        Increase linger.ms (wait a bit to form batches)
        Use compression (compression.type=lz4 or snappy)
        Set acks=1 if you can accept small durability risk
        Increase buffer.memory if you produce fast

    - Reliability (safe + still fast)
        Use acks=all
        Enable idempotence (enable.idempotence=true)
        Set reasonable retries and delivery.timeout.ms
    
    - Latency (minimize)
        Lower linger.ms
        Reduce batch.size
        Avoid heavy compression if CPU bound

10. What producer misconfigurations have you seen in production?
    Common Kafka producer misconfigurations I‚Äôve seen in production:
        acks=0 used unknowingly ‚Üí silent data loss
        Retries enabled but idempotence disabled ‚Üí duplicate messages
        acks=all with min.insync.replicas=1 ‚Üí weaker durability than expected
        Too small batch.size / linger.ms=0 ‚Üí poor throughput, high CPU/network overhead
        No compression (or wrong compression) ‚Üí high bandwidth + broker load
        Message too large vs max.request.size / broker limits ‚Üí send failures/timeouts
        Bad timeout settings (request.timeout.ms, delivery.timeout.ms) ‚Üí frequent timeouts/retries
        Too little buffer.memory ‚Üí producer blocks (buffer exhausted) under load
        Wrong partitioning (hot partition) ‚Üí high latency on one partition, uneven load
        Not handling backpressure ‚Üí app keeps producing while broker is slow, causing retries and spikes

        1) Messages missing (no errors)

        Symptom: Downstream says ‚Äúevents are missing,‚Äù producer logs look fine
        Root cause: acks=0 (fire-and-forget)
        Fix: Use acks=all (or at least acks=1) + monitoring for send errors

        2) Duplicate events in consumer/DB

        Symptom: Same order/trade/event appears twice
        Root cause: Retries enabled but idempotence off ‚Üí resend creates duplicates
        Fix: enable.idempotence=true; consider transactions if EOS needed

        3) ‚ÄúWe set acks=all but still lost data‚Äù

        Symptom: Producer got acks, but records disappeared after broker failure
        Root cause: acks=all but topic has min.insync.replicas=1 (weak durability)
        Fix: Set min.insync.replicas>=2 and keep replication factor ‚â• 3

        4) High producer latency spikes

        Symptom: P99 send latency jumps, app slows
        Root cause: Too much waiting for batches (linger.ms high) or broker/ISR slow with acks=all
        Fix: Reduce linger.ms if latency-sensitive; scale brokers/partitions; ensure ISR healthy

        5) Low throughput, high CPU, many small requests

        Symptom: Producer CPU high, network packets high, throughput low
        Root cause: batch.size too small and/or linger.ms=0 ‚Üí no batching
        Fix: Increase batch.size, set small linger.ms (e.g., 5‚Äì20ms), enable compression

        6) High bandwidth cost / broker CPU high

        Symptom: Network saturated, brokers CPU high
        Root cause: Compression disabled or inefficient choice
        Fix: Enable compression.type=lz4 (common balance) or zstd if CPU allows

        7) ‚ÄúRecordTooLarge‚Äù / send failures for some messages

        Symptom: Large events fail; small ones work
        Root cause: max.request.size (producer) / message.max.bytes (broker/topic) mismatch
        Fix: Align limits; avoid huge messages (store payload in S3, send pointer)

        8) Frequent timeouts and retry storms

        Symptom: TimeoutException, retries explode, latency worse
        Root cause: Too low request.timeout.ms / delivery.timeout.ms, or unstable broker/network
        Fix: Set sane timeouts; tune retries; fix broker/network bottleneck; keep idempotence on

        9) Producer ‚Äúblocks‚Äù or ‚Äúbuffer exhausted‚Äù

        Symptom: App threads hang on send; ‚Äúbuffer memory exhausted‚Äù
        Root cause: buffer.memory too small or broker too slow ‚Üí backpressure
        Fix: Increase buffer.memory; tune batching/compression; scale brokers; apply app-level backpressure

        10) One partition overloaded (hot partition)

        Symptom: Some consumers lag; one partition has huge traffic
        Root cause: Bad keying/partitioner (e.g., constant key), too few partitions
        Fix: Improve key distribution; add partitions; use sticky partitioning for keyless events


Consumers (Deep Dive)
0. What is heartbeat concept for consumer.
    Heartbeat in Kafka is how a consumer tells Kafka:
    üëâ ‚ÄúI‚Äôm alive and still part of the consumer group.‚Äù

    How it works:
        - Each consumer periodically sends heartbeats to the Group Coordinator broker.
        - As long as heartbeats arrive on time, the consumer keeps its partition assignment.
        - If heartbeats stop (crash, GC pause, network issue):
            - Coordinator marks the consumer dead
            - Triggers a rebalance
            - Partitions are reassigned to other consumers

    Important configs (interview favorites):
        - heartbeat.interval.ms ‚Üí how often heartbeats are sent
        - session.timeout.ms ‚Üí max time without heartbeat before consumer is considered dead
        - max.poll.interval.ms ‚Üí max time between poll() calls (processing watchdog)

1. How does Kafka consumer group coordination work?
    - When a consumer starts, it joins a consumer group and discovers the Group Coordinator 
       (a Kafka broker responsible for that group).
    - Consumers send JoinGroup requests; the coordinator collects all members.
    - A leader consumer (chosen by the coordinator) runs the partition assignment strategy 
       (range, round-robin, sticky).
    - The coordinator distributes assignments to all consumers.
    - Consumers start polling only their assigned partitions and send heartbeats to stay active.
    - If a consumer joins, leaves, or fails, the coordinator triggers a rebalance.
    Note : A consumer group is created on the Kafka broker (Group Coordinator) automatically 
    when the first consumer with a new group.id joins.
        - There is no explicit ‚Äúcreate consumer group‚Äù command.
        - When a consumer starts with a new group.id:
        - It sends a JoinGroup request to the broker acting as the Group Coordinator.
        - The coordinator creates group metadata in memory.
        - Offsets and group state are stored in the internal topic __consumer_offsets.
        - From then on, that group.id exists and is managed by the broker.

2. What causes consumer rebalancing?
    Consumer rebalancing in Kafka happens when the membership or assignment of a consumer group changes.
    Common causes include a consumer joining or leaving the group, a consumer failure or missed heartbeats, 
      topic partition changes, or configuration changes (like assignment strategy).
    When this happens, Kafka pauses consumption and redistributes partitions among active consumers.

3. Why are frequent rebalances dangerous?
    Frequent rebalances are dangerous because:
     - Consumption stops during rebalancing, directly impacting throughput
     - They increase consumer lag and end-to-end latency
     - They can cause duplicate processing or missed commits
     - They indicate an unstable consumer group, often due to bad configs or slow consumers

4. What is the role of poll() in Kafka consumers?
    while (running) {
        ConsumerRecords<String, String> records =
                consumer.poll(Duration.ofMillis(100));
        for (ConsumerRecord<String, String> record : records) {
            process(record);
        }
        consumer.commitSync(); // or async
    }
    - poll() is the core method that drives a Kafka consumer.
    - It fetches records from the broker and sends heartbeats to the group coordinator to 
            keep the consumer alive.
    - If poll() is not called within the configured interval, Kafka assumes the consumer 
        is dead and triggers a rebalance.
    Key configs tied to poll()
        max.poll.interval.ms ‚Üí max time between poll() calls
        max.poll.records ‚Üí how many records returned per poll()
        session.timeout.ms & heartbeat.interval.ms ‚Üí liveness checks
    In Spring Boot, you usually don‚Äôt call poll() yourself. Spring Kafka runs a background 
    consumer loop that continuously calls poll() for you.

5. What happens if poll() is delayed too long?
    If poll() is delayed too long:
        The consumer stops sending heartbeats
        The group coordinator marks it as dead
        A rebalance is triggered
        Its partitions are revoked and reassigned to other consumers
        The slow consumer may later rejoin, causing more rebalances and possible duplicates

6. How does max.poll.interval.ms affect consumers?
    - max.poll.interval.ms defines the maximum time a consumer can take to process records 
      between two poll() calls.
    - If processing takes longer than this value, Kafka assumes the consumer is stuck, 
      removes it from the group, and triggers a rebalance.

7. Difference between auto-commit and manual commit?
    - Auto-commit
        Kafka commits offsets automatically at intervals
        Simple to use, but can cause message loss or duplicates
        Less control over when a message is considered processed

    - Manual commit
        Application commits offsets explicitly after processing
        Gives full control and reliability
        Slightly more code, but safer for production

8. When can offsets be committed but processing fails?
    Offsets can be committed but processing can still fail in these cases:
    - Auto-commit enabled: Offsets are committed before processing completes, and the app crashes 
       during processing.
    - Manual commit done too early: The app commits offsets before writing to DB / downstream system, 
       then fails.
    - Async commit (commitAsync): Commit succeeds, but processing fails immediately after.
    - External system failure: Kafka offset is committed, but DB/API update fails.

9. How do you design idempotent consumers?
    To design idempotent consumers, make your processing safe to run more than once for the same message 
    (because Kafka is often at-least-once).

    Common patterns (used in production)
    1) Use a unique event id + dedup store
        Include eventId (UUID/orderId+version) in each message
        Before processing, check a table/cache: ‚Äúhave I processed this eventId?‚Äù
        If yes ‚Üí skip; if no ‚Üí process and record it

    2) Make the downstream write idempotent
        Use UPSERT / INSERT ‚Ä¶ ON CONFLICT DO NOTHING
        Use unique constraints on eventId or (businessKey, version)
        Avoid ‚Äúadd 10‚Äù updates; prefer ‚Äúset state to X‚Äù updates

    3) Transactional pattern (best)
        In one DB transaction:
        apply business update
        store eventId (or update a version)
        Commit Kafka offset after DB commit

    4) Use exactly-once tools when applicable
        Kafka Streams EOS / transactions can help for Kafka‚ÜíKafka pipelines, but DB still needs idempotency.

10.What consumer bugs have caused production incidents?
    Here are real consumer bugs that commonly cause production incidents, phrased the way interviewers like (symptom + root cause).
    - Rebalance storm: poll() blocked by long processing ‚Üí missed heartbeats / max.poll.interval.ms exceeded ‚Üí constant rebalances, no progress.
    - Duplicate processing: manual commit done after processing but downstream isn‚Äôt idempotent ‚Üí retries/rebalances reprocess messages and create duplicates.
    - Message loss: offsets committed too early (auto-commit or wrong manual commit timing) ‚Üí crash after commit ‚Üí records skipped forever.
    - Hot partition lag: bad keying (same key) ‚Üí one partition overloaded ‚Üí one consumer maxed out, group looks ‚Äúhealthy‚Äù but latency spikes.
    - Infinite retry loop: poison message keeps failing and is retried forever ‚Üí partition stuck, backlog grows.
    - OOM / GC pauses: fetching huge batches or large messages ‚Üí JVM GC stalls ‚Üí missed heartbeats ‚Üí rebalances + lag.
    - Threading mistake: using one KafkaConsumer across multiple threads ‚Üí unstable behavior, missed polls, exceptions, stalled consumption.
    - Wrong offset reset: misused auto.offset.reset (e.g., latest in a new group) ‚Üí starts after existing data ‚Üí ‚Äúmissing events‚Äù incident.
    - Bad error handling: swallowing exceptions and continuing commits ‚Üí silently drops failed records.
    - Under-provisioned consumers: too few consumers vs partitions or too slow processing ‚Üí lag grows until downstream SLAs break.


Offset Management & Delivery Semantics

1. What does ‚Äúat-least-once‚Äù delivery really mean?
    At-least-once delivery means:
        Every message will be delivered one or more times
        No message is lost
        Duplicates are possible

2. Why is ‚Äúexactly-once‚Äù often misunderstood in Kafka?
    It applies mainly to Kafka ‚Üí Kafka
        EOS guarantees cover producer retries + transactional writes between topics, not external systems.

    Offsets and side effects aren‚Äôt automatically atomic
        Writing to a DB/API and committing offsets are separate unless you design for it.

    Idempotence ‚â† exactly-once everywhere
        Idempotent producers prevent duplicate writes to Kafka, not duplicate processing.

    Upstream duplicates still exist
        If multiple producers emit the same business event, Kafka won‚Äôt dedupe them.

3. How does Kafka implement exactly-once semantics?
    Kafka implements exactly-once semantics (EOS) by combining idempotent producers + transactions + coordinated offset commits.
    How it works (step by step):
    Idempotent producer
        Each producer gets a Producer ID (PID) and adds sequence numbers.
        Broker drops duplicate retries ‚Üí no duplicate writes to Kafka.

    Transactions
        Producer starts a transaction (transactional.id).
        Writes to multiple partitions/topics are grouped atomically.
        On failure, the whole transaction is aborted.

    Atomic offset commit
        Consumer processes records.
        Offsets are sent to the transaction (sendOffsetsToTransaction()).
        commitTransaction() commits both output records and offsets together.

    Read committed
        Consumers with isolation.level=read_committed only see committed transactions.

4. When is exactly-once not worth the complexity?
    Use at-least-once instead if:

        Duplicates are harmless
            Logs, metrics, clickstreams, analytics ‚Üí easy to dedupe later.

        You write to external systems
            DBs / APIs where Kafka EOS doesn‚Äôt apply end-to-end anyway.

        High throughput / low latency is critical
            Transactions add coordination, latency, and operational overhead.

        Simple pipelines
            Single topic ‚Üí single consumer ‚Üí single sink (manual commit + idempotent write is enough).

        Operational simplicity matters
            Fewer configs, fewer failure modes, easier on-call life.

5. What happens if offsets are committed before DB writes?
    Consumer processes a record
    Offset is committed
    DB write fails or app crashes
    On restart, consumer resumes after that offset

6. How do you safely replay Kafka messages?
    Safely replaying Kafka messages means re-reading old data without breaking downstream.

    Use a new consumer group
        Replay with group.id=replay-<date> so prod consumers aren‚Äôt affected.

    Reset/seek offsets to the replay point
        Prefer by timestamp (e.g., ‚Äú3 days ago‚Äù) or to a known offset.

    Make downstream idempotent
        Dedupe by eventId or use DB unique constraint + upsert so repeats don‚Äôt corrupt.

    Throttle the replay
        Limit concurrency / max.poll.records, add rate limits to avoid overwhelming DB/services.

    Isolate output if risky
        Write to a staging topic/table first, validate, then merge/cut over.

7. How do you handle duplicate events?
    Handle duplicates by making processing idempotent:

        Include a unique eventId (or use (topic,partition,offset) as an ID).
        Deduplicate at the sink:
            DB unique constraint + upsert (INSERT ‚Ä¶ ON CONFLICT DO NOTHING/UPDATE)
            Or a processed_event table keyed by eventId.
        Make side effects idempotent (emails/payments): store a ‚Äúsent/processed‚Äù flag before sending.
        Producer safety: enable.idempotence=true to reduce duplicates from retries.
        Monitor: track dedupe-hit rate; sudden spikes indicate retries/rebalances.

8. How do you reset offsets in production safely?
    Stop consumers ‚Üí dry-run ‚Üí reset by timestamp/duration ‚Üí restart slowly with idempotent processing.

9. How do you reprocess historical data?
    Create a new replay consumer group (don‚Äôt touch prod group).
    Seek/reset to the time window you want (by timestamp / duration / offset).
    Run a dedicated replay job (separate deployment) with throttling.
    Ensure idempotency in downstream (eventId + upsert/unique constraint or dedupe table).
    Optionally write to a staging topic/table, validate, then merge/cut over.
    Monitor lag + error rate, and keep a quick stop switch.

10. What offset-related outages have you seen?
    Auto-commit caused data loss: offsets committed before processing/DB write ‚Üí missing records.
    Commit failures ignored: intermittent commit errors ‚Üí big reprocessing after restart ‚Üí duplicate writes / load spike.
    Accidental reset to earliest: consumer replays entire topic ‚Üí DB/API meltdown, lag explosion.
    Wrong group.id in prod: new group starts at latest/earliest unexpectedly ‚Üí either skips data or reprocesses too much.
    max.poll.interval.ms exceeded: slow processing ‚Üí consumer kicked out ‚Üí repeated rebalances + duplicates.
    Offset commit too frequent: commit on every record ‚Üí coordinator overload, latency increases.
    Offsets expired (offsets.retention.minutes): idle group comes back and starts from reset policy ‚Üí unexpected replay or skip.
    Rebalance storms: unstable consumers ‚Üí partitions move constantly ‚Üí duplicate processing + lag never recovers.

Topic & Partition Design

1. How do you decide the number of partitions?
    Decide partitions based on parallelism needed now + growth + broker limits:
    Required consumer parallelism
        Max active consumers in a group ‚âà #partitions
        Partitions ‚âà (target throughput) √∑ (throughput per partition)
    Ordering/key constraints
        If you need per-entity ordering, partitions must support expected key cardinality + hotspot risk.
    Producer throughput
        More partitions can increase write parallelism, but also overhead.
    Broker capacity / operational cost
        More partitions = more files, memory, leader/ISR work, rebalances.
        Stay within your cluster‚Äôs proven partition limits.
    Future scaling
        Add a buffer (often 2√ó) so you can scale consumers later without immediate repartitioning.

2. What happens if you under-partition a topic?
    Limited consumer parallelism
        Max active consumers in a group = #partitions ‚Üí scaling consumers won‚Äôt help.

    Throughput bottleneck
        One/few partitions become the choke point for produce + consume.

    Hot partition risk
        If most traffic hashes to one partition, it gets overloaded faster.

    Higher lag & latency
        Consumers can‚Äôt keep up ‚Üí lag grows, processing delays increase.

    Harder to scale later
        Adding partitions later can break per-key ordering and requires careful migration.

3. Can you change partition count later? What are the risks?
    Risks / side effects:

    Ordering can break for a key
        Key‚Üípartition mapping changes (hash(key)%N) ‚Üí same key may go to a different partition.

    Consumer rebalance
        Scaling partitions triggers rebalances ‚Üí temporary lag spikes.

    Hotspots can shift
        Existing ‚Äúhot key‚Äù may still stay hot; more partitions doesn‚Äôt always fix skew.

    Operational overhead increases
        More partitions = more files, memory, controller/metadata load.

    Stateful consumers become harder
        Apps relying on partition affinity/state stores may need changes.

4. How does partition count affect consumer scaling?
    Partition count directly caps consumer scaling.
        In a consumer group, only one consumer can read a partition at a time.
        So max parallel consumers = number of partitions.
        If consumers > partitions ‚Üí some consumers sit idle.
        If partitions > consumers ‚Üí consumers read multiple partitions each.

    Implications:
        Too few partitions ‚Üí can‚Äôt scale consumers ‚Üí lag grows.
        Enough partitions ‚Üí scale out consumers linearly (up to the partition count).
        More partitions also mean more rebalances and overhead, so don‚Äôt overdo it.

5. How do keys impact data distribution?
    Keys directly control how data is distributed across partitions.

    Same key ‚Üí same partition
        Kafka hashes the key ‚Üí guarantees ordering per key.

    High-cardinality keys ‚Üí even distribution
        Many unique keys spread load evenly ‚Üí good throughput.

    Low-cardinality / constant key ‚Üí hotspot
        Most data lands on 1‚Äì2 partitions ‚Üí lag + skew.

    Null key ‚Üí sticky/round-robin behavior
        No ordering guarantee; distribution evens out over time.

6. What happens with hot partitions?

7. How do you design topics for multi-tenant systems?

8. Topic-per-event vs shared-topic ‚Äî trade-offs?
    Topic-per-event
    Pros
        Simple consumers (no filtering)

        Different retention/ACLs per event type

        Easier to scale ‚Äúhot‚Äù event independently

    Cons
        Topic explosion ‚Üí ops overhead (partitions, configs, ACLs, monitoring)

        More metadata/controller load

        Harder governance and schema sprawl

    Shared topic (domain topic with eventType field/header)
    Pros
        Fewer topics ‚Üí easier governance/ops

        Better batching/compression

        Easier to add new event types without provisioning

    Cons
        Consumers must filter (wasted reads)

        Mixed retention/priority needs (hard to tune)

        No per-event isolation; noisy event can affect others

9. How do you handle topic explosion?

10. What topic design mistakes have you fixed?
    Here are common topic design mistakes I‚Äôve seen/fixed (good interview answers):
    Too few partitions ‚Üí lag/backlogs; fixed by increasing partitions + sharding keys / new topic migration.
    Hot keys / skewed distribution ‚Üí one partition overloaded; fixed by better keys, key-sharding, or tenant isolation.
    Topic-per-event / topic-per-tenant explosion ‚Üí unmanageable cluster; fixed by shared domain topics + governance + quotas.
    Wrong retention (too long/too short) ‚Üí disk fills or data missing; fixed with tiered retention + alerts.
    Using Kafka for blobs (huge payloads) ‚Üí OOM/disk issues; fixed by storing in S3/DB and sending references.
    Compaction used incorrectly ‚Üí ‚Äúmissing data‚Äù; fixed by proper keys, tombstone handling, and correct cleanup policy.
    Mixed workloads in one topic (critical + noisy) ‚Üí noisy neighbor; fixed by separating topics/quotas.
    No schema/versioning discipline ‚Üí consumer breaks; fixed via Schema Registry + compatibility rules.

Kafka Storage & Retention

1. How does Kafka store messages on disk?

2. What is log compaction?

3. When would you use log compaction over retention?

4. How does Kafka clean up old data?

5. What happens when disks fill up?

6. How do you size Kafka storage?

7. How does retention impact performance?

8. How do you handle large messages?

9. Why is Kafka not a database?

10. What data-loss scenarios exist in Kafka?

Schema & Data Evolution

1. How do you manage schema evolution in Kafka?

2. What happens when producers and consumers use different schemas?

3. How do you handle backward and forward compatibility?

4. Schema Registry ‚Äî why and when?

5. What schema mistakes cause outages?

6. How do you version events safely?

7. How do you deprecate fields in events?

8. What happens if schema compatibility rules are ignored?

9. How do you test schema changes?

10. What real schema evolution failures have you handled?

Error Handling & Reliability

1. How do you handle poison messages?

2. Retry topics vs dead-letter topics ‚Äî when to use what?

3. How do you avoid infinite retry loops?

4. How do you detect stuck partitions?

5. What happens when a consumer crashes mid-processing?

6. How do you monitor consumer lag correctly?

7. How do you design failure isolation?

8. How do you prevent cascading failures via Kafka?

9. How do you pause and resume consumption safely?

10. What reliability trade-offs have you made?

Performance & Tuning

What metrics matter most for Kafka performance?

How do you tune brokers for high throughput?

How do you tune consumers for low latency?

How do you detect network-related Kafka issues?

How does compression affect performance?

How do you prevent GC issues in Kafka clients?

What causes throughput to drop suddenly?

How do you benchmark Kafka?

What tuning changes caused outages?

What performance bottlenecks are hardest to diagnose?

Kafka in Distributed Systems

How do you design event-driven systems with Kafka?

Kafka vs REST ‚Äî when to use which?

How do you guarantee ordering across services?

How do you handle eventual consistency?

How do you manage inter-service contracts?

How do you design workflows using Kafka?

What anti-patterns exist in event-driven design?

How do you control blast radius with Kafka?

How do you handle cross-region Kafka?

What distributed system failures involved Kafka?

Kafka Operations & Production

How do you upgrade Kafka safely?

What breaks during Kafka upgrades?

How do you add brokers to a live cluster?

How do you rebalance partitions?

How do you detect broker failures?

How do you handle disk failures?

How do you secure Kafka in production?

How do you manage ACLs?

What Kafka incidents have you personally handled?

What Kafka decision do you regret?

Senior / Architect-Level Questions

When should Kafka NOT be used?

How do you design Kafka for 100+ services?

How do you prevent Kafka becoming a bottleneck?

How do you enforce governance around Kafka usage?

How do you handle multi-team ownership of topics?

How do you migrate from MQ to Kafka?

How do you justify Kafka operational cost?

How do you design DR for Kafka?

What would you redesign in your Kafka setup?

What Kafka failure taught you the most?