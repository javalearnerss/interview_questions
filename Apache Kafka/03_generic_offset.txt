

Offset Management & Delivery Semantics

1. What does “at-least-once” delivery really mean?
    At-least-once delivery means:
        Every message will be delivered one or more times
        No message is lost
        Duplicates are possible
        Note : At-least-once gaurantees that consumer will receive message at least once.
*******************************************************************************************************************************************

2. Why is “exactly-once” often misunderstood in Kafka?
   "Exactly once" delivery in Kafka means: Each message is delivered to the consumer and processed only one time (no duplicates, no loss).
    Achieved by combining idempotent producers and transactional messaging with careful consumer offset management.

    "Exactly-once" in Kafka is often misunderstood because:
    - Kafka guarantees "exactly-once" processing only for consume-process-produce workflows, not for simple producer-only or consumer-only cases.
    - For producers, enabling idempotence and transactions ensures each message is written only once to a topic.
    - For consumers, Kafka cannot guarantee "exactly-once" delivery; it guarantees "at least once" (possible duplicates) or "at most once" 
       (possible loss).
    - True "exactly-once" semantics require the consumer to process messages idempotently and commit offsets only after successful processing, 
        but duplicates may still occur if the consumer crashes before committing.
    - "Exactly-once" is only fully achieved when the consumer, processing logic, and producer are all coordinated using Kafka’s transactional APIs.

    Summary:
    Kafka's "exactly-once" is misunderstood because it applies to end-to-end processing (consume-process-produce with transactions), 
    not to simple producer or consumer delivery. Consumers must still handle possible duplicates.
*******************************************************************************************************************************************

3. How does Kafka implement exactly-once semantics?
    A message is processed only once, even if there are retries, failures, or restarts in producers, brokers, or consumers.

    1. Configure Kafka producer for idempotence and transactions.
    2. Configure Kafka consumer for manual offset commit and isolation.level=read_committed.
    3. Consumer reads messages.
    4. Process messages.
    5. Producer sends output messages within a transaction.
    6. Producer sends offsets to transaction.
    7. Commit transaction atomically (messages and offsets).
    8. If any error, abort transaction.

    public class ExactlyOnceConfig {

        @Bean
        public ProducerFactory<String, String> producerFactory() {
            Map<String, Object> props = new HashMap<>();
            props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
            props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
            props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
            props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
            // Do NOT set TRANSACTIONAL_ID_CONFIG directly; use prefix
            DefaultKafkaProducerFactory<String, String> factory =
                new DefaultKafkaProducerFactory<>(props);
            factory.setTransactionIdPrefix("exactly-once-transaction-");
            return factory;
        }

        @Bean
        public KafkaTemplate<String, String> kafkaTemplate() {
            return new KafkaTemplate<>(producerFactory());
        }

        @Bean
        public ConsumerFactory<String, String> consumerFactory() {
            Map<String, Object> props = new HashMap<>();
            props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
            props.put(ConsumerConfig.GROUP_ID_CONFIG, "exactly-once-group");
            props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
            props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");
            props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
            props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
            return new DefaultKafkaConsumerFactory<>(props);
        }

        @Bean
        public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory() {
            ConcurrentKafkaListenerContainerFactory<String, String> factory =
                new ConcurrentKafkaListenerContainerFactory<>();

            factory.setConsumerFactory(consumerFactory());
            factory.getContainerProperties().setAckMode(
                org.springframework.kafka.listener.ContainerProperties.AckMode.MANUAL
            );

            return factory;
        }
    }


        @Service
        public class ExactlyOnceListener {

            @Autowired
            private KafkaTemplate<String, String> kafkaTemplate;

            @KafkaListener(topics = "input-topic", containerFactory = "kafkaListenerContainerFactory")
            public void listen(
                ConsumerRecord<String, String> record,
                Acknowledgment ack,
                Consumer<String, String> consumer
            ) {

                kafkaTemplate.executeInTransaction(kt -> {

                    // Process the record
                    String processedValue = process(record.value());

                    // Produce to output topic
                    kt.send("output-topic", record.key(), processedValue);

                    // Prepare offset map for commit
                    Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();
                    offsets.put(
                        new TopicPartition(record.topic(), record.partition()),
                        new OffsetAndMetadata(record.offset() + 1)
                    );

                    // Commit offset as part of transaction
                    consumer.commitSync(offsets);

                    return null;
                });

                ack.acknowledge();
            }

            private String process(String value) {
                // Business logic here
                return value.toUpperCase();
            }
        }



*******************************************************************************************************************************************

4. When is exactly-once not worth the complexity?
    Use at-least-once instead if:

        Duplicates are harmless
            Logs, metrics, clickstreams, analytics → easy to dedupe later.

        You write to external systems
            DBs / APIs where Kafka EOS doesn’t apply end-to-end anyway.

        High throughput / low latency is critical
            Transactions add coordination, latency, and operational overhead.

        Simple pipelines
            Single topic → single consumer → single sink (manual commit + idempotent write is enough).

        Operational simplicity matters
            Fewer configs, fewer failure modes, easier on-call life.

*******************************************************************************************************************************************

5. What happens if offsets are committed before DB writes?
    Consumer processes a record
    Offset is committed
    DB write fails or app crashes
    On restart, consumer resumes after that offset

*******************************************************************************************************************************************

6. How do you safely replay Kafka messages?
    Safely replaying Kafka messages means re-reading old data without breaking downstream.

    Use a new consumer group
        Replay with group.id=replay-<date> so prod consumers aren’t affected.

    Reset/seek offsets to the replay point
        Prefer by timestamp (e.g., “3 days ago”) or to a known offset.

    Make downstream idempotent
        Dedupe by eventId or use DB unique constraint + upsert so repeats don’t corrupt.

    Throttle the replay
        Limit concurrency / max.poll.records, add rate limits to avoid overwhelming DB/services.

    Isolate output if risky
        Write to a staging topic/table first, validate, then merge/cut over.

*******************************************************************************************************************************************

7. How do you handle duplicate events?
    Handle duplicates by making processing idempotent:

        Include a unique eventId (or use (topic,partition,offset) as an ID).
        Deduplicate at the sink:
            DB unique constraint + upsert (INSERT … ON CONFLICT DO NOTHING/UPDATE)
            Or a processed_event table keyed by eventId.
        Make side effects idempotent (emails/payments): store a “sent/processed” flag before sending.
        Producer safety: enable.idempotence=true to reduce duplicates from retries.
        Monitor: track dedupe-hit rate; sudden spikes indicate retries/rebalances.

*******************************************************************************************************************************************

8. How do you reset offsets in production safely?
    Stop consumers → dry-run → reset by timestamp/duration → restart slowly with idempotent processing.

*******************************************************************************************************************************************

9. What offset-related outages have you seen?
    Auto-commit caused data loss: offsets committed before processing/DB write → missing records.
    Commit failures ignored: intermittent commit errors → big reprocessing after restart → duplicate writes / load spike.
    Accidental reset to earliest: consumer replays entire topic → DB/API meltdown, lag explosion.
    Wrong group.id in prod: new group starts at latest/earliest unexpectedly → either skips data or reprocesses too much.
    max.poll.interval.ms exceeded: slow processing → consumer kicked out → repeated rebalances + duplicates.
    Offset commit too frequent: commit on every record → coordinator overload, latency increases.
    Offsets expired (offsets.retention.minutes): idle group comes back and starts from reset policy → unexpected replay or skip.
    Rebalance storms: unstable consumers → partitions move constantly → duplicate processing + lag never recovers.

*******************************************************************************************************************************************
