
Topic & Partition Design

1. How do you decide the number of partitions?
    Decide partitions based on parallelism needed now + growth + broker limits:
   
    Required consumer parallelism
        Max active consumers in a group ≈ #partitions
        Partitions ≈ (target throughput) ÷ (throughput per partition)
    Ordering/key constraints
        If you need per-entity ordering, partitions must support expected key cardinality + hotspot risk.
    Producer throughput
        More partitions can increase write parallelism, but also overhead.
    Broker capacity / operational cost
        More partitions = more files, memory, leader/ISR work, rebalances.
        Stay within your cluster’s proven partition limits.
    Future scaling
        Add a buffer (often 2×) so you can scale consumers later without immediate repartitioning.

2. What happens if you under-partition a topic?
    Limited consumer parallelism
        Max active consumers in a group = #partitions → scaling consumers won’t help.

    Throughput bottleneck
        One/few partitions become the choke point for produce + consume.

    Hot partition risk
        If most traffic hashes to one partition, it gets overloaded faster.

    Higher lag & latency
        Consumers can’t keep up → lag grows, processing delays increase.

    Harder to scale later
        Adding partitions later can break per-key ordering and requires careful migration.

3. Can you change partition count later? What are the risks?
    Risks / side effects:

    Ordering can break for a key
        Key→partition mapping changes (hash(key)%N) → same key may go to a different partition.

    Consumer rebalance
        Scaling partitions triggers rebalances → temporary lag spikes.

    Hotspots can shift
        Existing “hot key” may still stay hot; more partitions doesn’t always fix skew.

    Operational overhead increases
        More partitions = more files, memory, controller/metadata load.

    Stateful consumers become harder
        Apps relying on partition affinity/state stores may need changes.

4. How does partition count affect consumer scaling?
    Partition count directly caps consumer scaling.
        In a consumer group, only one consumer can read a partition at a time.
        So max parallel consumers = number of partitions.
        If consumers > partitions → some consumers sit idle.
        If partitions > consumers → consumers read multiple partitions each.

    Implications:
        Too few partitions → can’t scale consumers → lag grows.
        Enough partitions → scale out consumers linearly (up to the partition count).
        More partitions also mean more rebalances and overhead, so don’t overdo it.

5. How do keys impact data distribution?
    Keys directly control how data is distributed across partitions.

    Same key → same partition
        Kafka hashes the key → guarantees ordering per key.

    High-cardinality keys → even distribution
        Many unique keys spread load evenly → good throughput.

    Low-cardinality / constant key → hotspot
        Most data lands on 1–2 partitions → lag + skew.

    Null key → sticky/round-robin behavior
        No ordering guarantee; distribution evens out over time.

6. What happens with hot partitions?

7. How do you design topics for multi-tenant systems?

8. Topic-per-event vs shared-topic — trade-offs?
    Topic-per-event
    Pros
        Simple consumers (no filtering)
        Different retention/ACLs per event type
        Easier to scale “hot” event independently

    Cons
        Topic explosion → ops overhead (partitions, configs, ACLs, monitoring)
        More metadata/controller load
        Harder governance and schema sprawl

    Shared topic (domain topic with eventType field/header)
    Pros
        Fewer topics → easier governance/ops
        Better batching/compression
        Easier to add new event types without provisioning

    Cons
        Consumers must filter (wasted reads)
        Mixed retention/priority needs (hard to tune)
        No per-event isolation; noisy event can affect others



9. What topic design mistakes have you fixed?
    Here are common topic design mistakes I’ve seen/fixed (good interview answers):
    Too few partitions → lag/backlogs; fixed by increasing partitions + sharding keys / new topic migration.
    Hot keys / skewed distribution → one partition overloaded; fixed by better keys, key-sharding, or tenant isolation.
    Topic-per-event / topic-per-tenant explosion → unmanageable cluster; fixed by shared domain topics + governance + quotas.
    Wrong retention (too long/too short) → disk fills or data missing; fixed with tiered retention + alerts.
    Using Kafka for blobs (huge payloads) → OOM/disk issues; fixed by storing in S3/DB and sending references.
    Compaction used incorrectly → “missing data”; fixed by proper keys, tombstone handling, and correct cleanup policy.
    Mixed workloads in one topic (critical + noisy) → noisy neighbor; fixed by separating topics/quotas.
    No schema/versioning discipline → consumer breaks; fixed via Schema Registry + compatibility rules.
