Producers (Deep Dive)

Note : Idempotence in a Kafka producer means:
    ðŸ‘‰ If the producer sends the same message multiple times (due to retries, network errors, etc.), 
    Kafka ensures it is written only once to the topic partition.


0. When are Producer ID (PID) and sequence numbers assigned?
    Producer ID (PID)
        Assigned once, when the producer starts and idempotence is enabled
        Happens on the first request to the broker (InitProducerId request)
        Broker returns a unique PID for that producer instance
    Sequence numbers
        Assigned by the producer client, not the broker
        Start at 0 per (PID + topic-partition)
        Incremented for every record sent to that partition
    Flow (simple):
        Producer starts with enable.idempotence=true
        Producer contacts broker â†’ broker assigns PID
        Producer sends records:
        Each record gets a sequence number per partition
        Broker tracks (PID, partition, sequence)
        If a retry arrives with the same sequence number â†’ broker drops it as duplicate

1.  What delivery guarantees does Kafka provide?
    Kafka supports at-most-once, at-least-once, and exactly-once delivery semantics, 
    depending on producer, consumer, and transaction configuration.
        At-most-once: Messages are processed at most once and may be lost but never duplicated.
        At-least-once: Messages are never lost but may be processed more than once.
        Exactly-once: Messages are processed once and only once, with no loss or duplication.

2.  How do acks=0, acks=1, and acks=all differ in practice?
    acks=0: Producer does not wait for any broker response â†’ fastest, but messages can be lost silently.
    acks=1: Producer waits for leader acknowledgment only â†’ good balance, but data can be lost if the 
            leader fails before replication.
    acks=all: Producer waits for all in-sync replicas to acknowledge â†’ strongest durability, slower throughput.

3.  When can Kafka acknowledge a message but still lose it?
    Kafka can acknowledge a message but still lose it when acks=1 and the leader fails before the 
    message is replicated to in-sync replicas.

4.  How do retries work in Kafka producers?
    When a producer sends a message and doesnâ€™t receive an acknowledgment 
    (due to a transient error like leader change or network issue), 
    Kafka automatically retries sending the same message.
        The producer retries up to retries times
        Retries are spaced by retry.backoff.ms
        Retries happen only for retriable errors (not for invalid messages)

    Important side effect
    If retries are enabled without idempotence, the same message can be written multiple times, 
    causing duplicates.

    Best practice
    Enable idempotent producer (enable.idempotence=true)
    Use acks=all
    This ensures retries do not create duplicates.


5.  How do you avoid message duplication on producer retries?
    To avoid duplication when producer retries happen in Kafka:
    - Enable idempotent producer (enable.idempotence=true) so Kafka deduplicates retries using 
      producer ID + sequence numbers.
    - Use acks=all (and keep a proper min.insync.replicas) so acks mean the record is safely replicated.
    - If you need exactly-once across topics/processing, use transactions (transactional.id) 
      with read_committed consumers.

6.  What is idempotent producer and when do you enable it?
    An idempotent producer ensures that even if a producer retries sending a message, 
    Kafka writes it only onceâ€”no duplicates.
    - How it works: Kafka assigns a producer ID and sequence numbers to records; 
      the broker detects and drops duplicate retries.

    - When to enable it:
      Enable it whenever reliability matters and retries are possible, especially with acks=all, 
      leader failovers, or unstable networksâ€”common in production systems.

7.  How does batching improve producer throughput?
    Batching lets a Kafka producer accumulate multiple records and send them together in a single 
    request instead of sending each message individually. It is reducing network and broker overhead.

8.  What are common causes of producer latency?
    Latency = the time taken for a request to go from sender â†’ processing â†’ response.
        High acks setting (especially acks=all) â†’ producer waits for replica confirmations
        Large batch wait time (linger.ms) â†’ producer delays send to fill batches
        Broker load or slow disk I/O â†’ brokers take longer to write and acknowledge
        Network latency / throttling â†’ slow producerâ€“broker communication
        ISR issues (slow or shrinking replicas) â†’ delays acknowledgments
        Retries due to transient errors â†’ resend adds extra time
        Large message size â†’ more time to serialize, transfer, and write

9.  How do you tune producer performance?
    Latency is how long something takes; throughput is how much you can do per unit time.
    To tune Kafka producer performance, focus on throughput vs latency trade-offs:
    - Throughput (maximize)
        Increase batch.size (bigger batches)
        Increase linger.ms (wait a bit to form batches)
        Use compression (compression.type=lz4 or snappy)
        Set acks=1 if you can accept small durability risk
        Increase buffer.memory if you produce fast

    - Reliability (safe + still fast)
        Use acks=all
        Enable idempotence (enable.idempotence=true)
        Set reasonable retries and delivery.timeout.ms
    
    - Latency (minimize)
        Lower linger.ms
        Reduce batch.size
        Avoid heavy compression if CPU bound

10. What producer misconfigurations have you seen in production?
    Common Kafka producer misconfigurations Iâ€™ve seen in production:
        acks=0 used unknowingly â†’ silent data loss
        Retries enabled but idempotence disabled â†’ duplicate messages
        acks=all with min.insync.replicas=1 â†’ weaker durability than expected
        Too small batch.size / linger.ms=0 â†’ poor throughput, high CPU/network overhead
        No compression (or wrong compression) â†’ high bandwidth + broker load
        Message too large vs max.request.size / broker limits â†’ send failures/timeouts
        Bad timeout settings (request.timeout.ms, delivery.timeout.ms) â†’ frequent timeouts/retries
        Too little buffer.memory â†’ producer blocks (buffer exhausted) under load
        Wrong partitioning (hot partition) â†’ high latency on one partition, uneven load
        Not handling backpressure â†’ app keeps producing while broker is slow, causing retries and spikes

        1) Messages missing (no errors)

        Symptom: Downstream says â€œevents are missing,â€ producer logs look fine
        Root cause: acks=0 (fire-and-forget)
        Fix: Use acks=all (or at least acks=1) + monitoring for send errors

        2) Duplicate events in consumer/DB

        Symptom: Same order/trade/event appears twice
        Root cause: Retries enabled but idempotence off â†’ resend creates duplicates
        Fix: enable.idempotence=true; consider transactions if EOS needed

        3) â€œWe set acks=all but still lost dataâ€

        Symptom: Producer got acks, but records disappeared after broker failure
        Root cause: acks=all but topic has min.insync.replicas=1 (weak durability)
        Fix: Set min.insync.replicas>=2 and keep replication factor â‰¥ 3

        4) High producer latency spikes

        Symptom: P99 send latency jumps, app slows
        Root cause: Too much waiting for batches (linger.ms high) or broker/ISR slow with acks=all
        Fix: Reduce linger.ms if latency-sensitive; scale brokers/partitions; ensure ISR healthy

        5) Low throughput, high CPU, many small requests

        Symptom: Producer CPU high, network packets high, throughput low
        Root cause: batch.size too small and/or linger.ms=0 â†’ no batching
        Fix: Increase batch.size, set small linger.ms (e.g., 5â€“20ms), enable compression

        6) High bandwidth cost / broker CPU high

        Symptom: Network saturated, brokers CPU high
        Root cause: Compression disabled or inefficient choice
        Fix: Enable compression.type=lz4 (common balance) or zstd if CPU allows

        7) â€œRecordTooLargeâ€ / send failures for some messages

        Symptom: Large events fail; small ones work
        Root cause: max.request.size (producer) / message.max.bytes (broker/topic) mismatch
        Fix: Align limits; avoid huge messages (store payload in S3, send pointer)

        8) Frequent timeouts and retry storms

        Symptom: TimeoutException, retries explode, latency worse
        Root cause: Too low request.timeout.ms / delivery.timeout.ms, or unstable broker/network
        Fix: Set sane timeouts; tune retries; fix broker/network bottleneck; keep idempotence on

        9) Producer â€œblocksâ€ or â€œbuffer exhaustedâ€

        Symptom: App threads hang on send; â€œbuffer memory exhaustedâ€
        Root cause: buffer.memory too small or broker too slow â†’ backpressure
        Fix: Increase buffer.memory; tune batching/compression; scale brokers; apply app-level backpressure

        10) One partition overloaded (hot partition)

        Symptom: Some consumers lag; one partition has huge traffic
        Root cause: Bad keying/partitioner (e.g., constant key), too few partitions
        Fix: Improve key distribution; add partitions; use sticky partitioning for keyless events


*************************************************************************************************

Que.1 -> How to prevent duplicate messages in kafka producer.
    Implement "exactly once" producer:
        1. Enable idempotence in producer config.
        2. Set a unique transactional ID.
        3. Initialize transactions.
        4. Begin transaction, send messages, commit transaction.
        5. If any error occurs, abort transaction.

        @Configuration
        public class KafkaProducerConfig {

            @Bean  
            public ProducerFactory<String, String> producerFactory() {  
                Map<String, Object> props = new HashMap<>();  
                props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");  
                props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);  
                props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);  
                props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);  

                DefaultKafkaProducerFactory<String, String> factory = new DefaultKafkaProducerFactory<>(props);  
                factory.setTransactionIdPrefix("my-app-transactional-id-");   Kafka appends a unique suffix (e.g., my-app-transactional-id-0, my-app-transactional-id-1)
                return factory;  
            }  

            @Bean  
            public KafkaTemplate<String, String> kafkaTemplate() {  
                return new KafkaTemplate<>(producerFactory());  
            }  

        }

        factory.setTransactionIdPrefix("my-app-transactional-id-")
        - Kafka creates a unique transactional ID per producer instance (not per transaction).
        - Spring Kafka appends a unique suffix (e.g., my-app-transactional-id-0, my-app-transactional-id-1) for each producer 
          instance created by the factory.
        - All transactions started by the same producer instance use the same transactional ID.
        - This ensures transactional ID uniqueness across concurrent producers, but not per individual transaction.

        @Service
        public class TransactionalProducerService {

            @Autowired
            private KafkaTemplate<String, String> kafkaTemplate;

            public void sendMessagesExactlyOnce() {
                kafkaTemplate.executeInTransaction(kt -> {
                    kt.send("your-topic", "key1", "value1");
                    kt.send("your-topic", "key2", "value2");
                    // Add more sends as needed
                    return true;
                });
            }
        }
