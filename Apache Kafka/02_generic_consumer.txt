
Consumers (Deep Dive)
0. What is heartbeat concept for consumer.
    Heartbeat in Kafka is how a consumer tells Kafka:
    üëâ ‚ÄúI‚Äôm alive and still part of the consumer group.‚Äù

    How it works:
        - Each consumer periodically sends heartbeats to the Group Coordinator broker.
        - As long as heartbeats arrive on time, the consumer keeps its partition assignment.
        - If heartbeats stop (crash, GC pause, network issue):
            - Coordinator marks the consumer dead
            - Triggers a rebalance
            - Partitions are reassigned to other consumers

    Important configs (interview favorites):
        - heartbeat.interval.ms ‚Üí how often heartbeats are sent
        - session.timeout.ms ‚Üí max time without heartbeat before consumer is considered dead
        - max.poll.interval.ms ‚Üí max time between poll() calls (processing watchdog)

1. How does Kafka consumer group coordination work?
    - When a consumer starts, it joins a consumer group and discovers the Group Coordinator 
       (a Kafka broker responsible for that group).
    - Consumers send JoinGroup requests; the coordinator collects all members.
    - A leader consumer (chosen by the coordinator) runs the partition assignment strategy 
       (range, round-robin, sticky).
    - The coordinator distributes assignments to all consumers.
    - Consumers start polling only their assigned partitions and send heartbeats to stay active.
    - If a consumer joins, leaves, or fails, the coordinator triggers a rebalance.
    Note : A consumer group is created on the Kafka broker (Group Coordinator) automatically 
    when the first consumer with a new group.id joins.
        - There is no explicit ‚Äúcreate consumer group‚Äù command.
        - When a consumer starts with a new group.id:
        - It sends a JoinGroup request to the broker acting as the Group Coordinator.
        - The coordinator creates group metadata in memory.
        - Offsets and group state are stored in the internal topic __consumer_offsets.
        - From then on, that group.id exists and is managed by the broker.

2. What causes consumer rebalancing?
    Consumer rebalancing in Kafka happens when the membership or assignment of a consumer group changes.
    Common causes include a consumer joining or leaving the group, a consumer failure or missed heartbeats, 
      topic partition changes, or configuration changes (like assignment strategy).
    When this happens, Kafka pauses consumption and redistributes partitions among active consumers.

3. Why are frequent rebalances dangerous?
    Frequent rebalances are dangerous because:
     - Consumption stops during rebalancing, directly impacting throughput
     - They increase consumer lag and end-to-end latency
     - They can cause duplicate processing or missed commits
     - They indicate an unstable consumer group, often due to bad configs or slow consumers

4. What is the role of poll() in Kafka consumers?
    while (running) {
        ConsumerRecords<String, String> records =
                consumer.poll(Duration.ofMillis(100));
        for (ConsumerRecord<String, String> record : records) {
            process(record);
        }
        consumer.commitSync(); // or async
    }
    - poll() is the core method that drives a Kafka consumer.
    - It fetches records from the broker and sends heartbeats to the group coordinator to 
            keep the consumer alive.
    - If poll() is not called within the configured interval, Kafka assumes the consumer 
        is dead and triggers a rebalance.
    Key configs tied to poll()
        max.poll.interval.ms ‚Üí max time between poll() calls
        max.poll.records ‚Üí how many records returned per poll()
        session.timeout.ms & heartbeat.interval.ms ‚Üí liveness checks
    In Spring Boot, you usually don‚Äôt call poll() yourself. Spring Kafka runs a background 
    consumer loop that continuously calls poll() for you.

5. What happens if poll() is delayed too long?
    If poll() is delayed too long:
        The consumer stops sending heartbeats
        The group coordinator marks it as dead
        A rebalance is triggered
        Its partitions are revoked and reassigned to other consumers
        The slow consumer may later rejoin, causing more rebalances and possible duplicates

6. How does max.poll.interval.ms affect consumers?
    - max.poll.interval.ms defines the maximum time a consumer can take to process records 
      between two poll() calls.
    - If processing takes longer than this value, Kafka assumes the consumer is stuck, 
      removes it from the group, and triggers a rebalance.

7. Difference between auto-commit and manual commit?
    - Auto-commit
        Kafka commits offsets automatically at intervals
        Simple to use, but can cause message loss or duplicates
        Less control over when a message is considered processed

    - Manual commit
        Application commits offsets explicitly after processing
        Gives full control and reliability
        Slightly more code, but safer for production

8. When can offsets be committed but processing fails?
    Offsets can be committed but processing can still fail in these cases:
    - Auto-commit enabled: Offsets are committed before processing completes, and the app crashes 
       during processing.
    - Manual commit done too early: The app commits offsets before writing to DB / downstream system, 
       then fails.
    - Async commit (commitAsync): Commit succeeds, but processing fails immediately after.
    - External system failure: Kafka offset is committed, but DB/API update fails.

9. How do you design idempotent consumers?
    To design idempotent consumers, make your processing safe to run more than once for the same message 
    (because Kafka is often at-least-once).

    Common patterns (used in production)
    1) Use a unique event id + dedup store
        Include eventId (UUID/orderId+version) in each message
        Before processing, check a table/cache: ‚Äúhave I processed this eventId?‚Äù
        If yes ‚Üí skip; if no ‚Üí process and record it

    2) Make the downstream write idempotent
        Use UPSERT / INSERT ‚Ä¶ ON CONFLICT DO NOTHING
        Use unique constraints on eventId or (businessKey, version)
        Avoid ‚Äúadd 10‚Äù updates; prefer ‚Äúset state to X‚Äù updates

    3) Transactional pattern (best)
        In one DB transaction:
        apply business update
        store eventId (or update a version)
        Commit Kafka offset after DB commit

    4) Use exactly-once tools when applicable
        Kafka Streams EOS / transactions can help for Kafka‚ÜíKafka pipelines, but DB still needs idempotency.

10.What consumer bugs have caused production incidents?
    Here are real consumer bugs that commonly cause production incidents, phrased the way interviewers like (symptom + root cause).
    - Rebalance storm: poll() blocked by long processing ‚Üí missed heartbeats / max.poll.interval.ms exceeded ‚Üí constant rebalances, no progress.
    - Duplicate processing: manual commit done after processing but downstream isn‚Äôt idempotent ‚Üí retries/rebalances reprocess messages and create duplicates.
    - Message loss: offsets committed too early (auto-commit or wrong manual commit timing) ‚Üí crash after commit ‚Üí records skipped forever.
    - Hot partition lag: bad keying (same key) ‚Üí one partition overloaded ‚Üí one consumer maxed out, group looks ‚Äúhealthy‚Äù but latency spikes.
    - Infinite retry loop: poison message keeps failing and is retried forever ‚Üí partition stuck, backlog grows.
    - OOM / GC pauses: fetching huge batches or large messages ‚Üí JVM GC stalls ‚Üí missed heartbeats ‚Üí rebalances + lag.
    - Threading mistake: using one KafkaConsumer across multiple threads ‚Üí unstable behavior, missed polls, exceptions, stalled consumption.
    - Wrong offset reset: misused auto.offset.reset (e.g., latest in a new group) ‚Üí starts after existing data ‚Üí ‚Äúmissing events‚Äù incident.
    - Bad error handling: swallowing exceptions and continuing commits ‚Üí silently drops failed records.
    - Under-provisioned consumers: too few consumers vs partitions or too slow processing ‚Üí lag grows until downstream SLAs break.

******************************************************************************************************************************

**** Kafka Consumer exactly-one semantic***
- Kafka itself does not provide true "exactly once" delivery to a consumer-only application; it provides "at least once" or "at most once".
- "Exactly once" semantics require the consumer's processing to be idempotent, and the consumer must commit offsets only after successful processing.
- Set consumer isolation level to read_committed if upstream producers use transactions.
- Use manual offset commit (disable auto-commit).
- Ensure your processing logic is idempotent to avoid side effects from possible duplicate deliveries.


    @Configuration
    @EnableKafka
    public class ExactlyOnceConsumerConfig {

        @Bean
        public ConsumerFactory<String, String> consumerFactory() {
            Map<String, Object> props = new HashMap<>();
            props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
            props.put(ConsumerConfig.GROUP_ID_CONFIG, "exactly-once-group");
            props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); // manual commit
            props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed"); // only if producer uses transactions
            props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
            props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);

            return new DefaultKafkaConsumerFactory<>(props);
        }
    }

    // Consumer

    @RetryableTopic(
        backoff = @Backoff(value = 3000L, multiplierExpression = "2"),
        attempts = "4", // N-1 Retry attempts
        topicSuffixingStrategy = TopicSuffixingStrategy.SUFFIX_WITH_INDEX_VALUE,
        autoCreateTopics = "false",
        dltTopicSuffix = "-dlt",
        dltStrategy = DltStrategy.FAIL_ON_ERROR,
        include = DelayException.class
    )
    @KafkaListener(
        topics = "${topic-name}",
        groupId = "${groupid}"
    )
    public void consume(
        @Payload String message,
        @Header(value = KafkaHeaders.OFFSET, required = false) Long offset,
        @Header(value = KafkaHeaders.RECEIVED_TOPIC, required = false) String topic,
        @Headers MessageHeaders headers,
        Acknowledgment ack
    ) {

        LOGGER.info("#### -> Received message {} from topic {} with offset {}", message, topic, offset);

        try {
            

            bigMacMessageProcessor.processMessage(message);

        } catch (final DelayException delayExp) {
            LOGGER.error("Retrying message due to DelayException: {}", delayExp.getMessage());
            throw delayExp;

        } catch (Exception ex) {
            LOGGER.error("Moving message to DLT due to exception: {}", ex.getMessage());
            throw new Exception(ex);
        }

        ack.acknowledge();
    }

********************************************************************************************************************

Consumer concurrency (4 threads in one process) = one Kafka consumer instance internally processing in parallel.

4 separate processes (each with 1 consumer) = four independent Kafka consumer instances working in parallel.

Kafka View: Group Membership & Partitions
    - Consumer Concurrency (Single Process, 4 Threads)
        A Kafka application runs one consumer instance inside a single process.
        This consumer polls records from its assigned partitions and pushes them into an internal work queue.

        Inside the same process, four worker threads pull records from that queue and process them in parallel.

        From Kafka‚Äôs perspective, the application is one group member, so partition assignment happens only once ‚Äî to that single consumer.

        Implication:
        Throughput can improve for processing, but fetch parallelism is limited to partitions assigned to that one consumer instance.
    
    - 4 Separate Processes (4 Consumers)
        A Kafka deployment runs 4 separate processes, each with one consumer instance.

        Each process joins the same consumer group, so Kafka sees 4 group members.
        Kafka distributes partitions across all four consumers automatically.

Fault Isolation
    - Consumer Concurrency (Single Process, 4 Threads)
        If the process crashes, the consumer and all 4 workers stop, and Kafka must rebalance partitions.

    - 4 Separate Processes (4 Consumers)
        If one process crashes, only that consumer is lost ‚Äî the other three continue processing without interruption.

********************************************************************************************************************
Kafka comsumer common properties


| Property Name           | Description                                                                                             |
| ----------------------- | ------------------------------------------------------------------------------------------------------- |
| bootstrap.servers       | Comma-separated list of Kafka broker addresses.                                                         |
| group.id                | Consumer group identifier.                                                                              |
| key.deserializer        | Deserializer class for message keys (e.g., `org.apache.kafka.common.serialization.StringDeserializer`). |
| value.deserializer      | Deserializer class for message values.                                                                  |
| enable.auto.commit      | If true, consumer‚Äôs offset is periodically committed automatically.                                     |
| auto.commit.interval.ms | Frequency (ms) of auto-commit if enabled.                                                               |
| auto.offset.reset       | What to do when there is no initial offset (earliest, latest, none).                                    |
| fetch.min.bytes         | Minimum amount of data the server should return for a fetch request.                                    |
| fetch.max.bytes         | Maximum amount of data the server should return for a fetch request.                                    |
| max.poll.records        | Maximum records returned in a single poll.                                                              |
| max.poll.interval.ms    | Maximum delay between invocations of poll().                                                            |
| session.timeout.ms      | Timeout used to detect consumer failures.                                                               |
| heartbeat.interval.ms   | Interval for sending heartbeats to the broker.                                                          |
| client.id               | Logical identifier for the consumer.                                                                    |
| security.protocol       | Security protocol (PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL).                                           |
| ssl.*                   | SSL configuration properties.                                                                           |
| sasl.*                  | SASL configuration properties.                                                                          |


*******************************************************************************************