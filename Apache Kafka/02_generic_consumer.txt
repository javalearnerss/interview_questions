
Consumers (Deep Dive)
0. What is heartbeat concept for consumer.
    Heartbeat in Kafka is how a consumer tells Kafka:
    üëâ ‚ÄúI‚Äôm alive and still part of the consumer group.‚Äù

    How it works:
        - Each consumer periodically sends heartbeats to the Group Coordinator broker.
        - As long as heartbeats arrive on time, the consumer keeps its partition assignment.
        - If heartbeats stop (crash, GC pause, network issue):
            - Coordinator marks the consumer dead
            - Triggers a rebalance
            - Partitions are reassigned to other consumers

    Important configs (interview favorites):
        - heartbeat.interval.ms ‚Üí how often heartbeats are sent
        - session.timeout.ms ‚Üí max time without heartbeat before consumer is considered dead
        - max.poll.interval.ms ‚Üí max time between poll() calls (processing watchdog)

1. How does Kafka consumer group coordination work?
    - When a consumer starts, discovers the Group Coordinator (a Kafka broker responsible for that group).
    - Consumers send JoinGroup requests; if consumer group is not present then the coordinator creates the group. 
      If group is already present then consumer joins the group and group coordinator triggers the rebalance
    - Coordinator elects a consumer leader.
    - A leader consumer (chosen by the coordinator) runs the partition assignment strategy (range, round-robin, sticky) and sends assignment plan to Coordinator
    - The coordinator distributes assignments to all consumers.
    - Consumers start polling only their assigned partitions and send heartbeats to stay active.
    - If a consumer joins, leaves, or fails, the coordinator triggers a rebalance.
    Note : A consumer group is created on the Kafka broker (Group Coordinator) automatically 
    

    First Consumer
        Consumer-1 starts  ‚Üí Connects to Broker ‚Üí Find Coordinator  ‚Üí JoinGroup  ‚Üí Group Created  
        ‚Üí Leader = Consumer-1(It has Topic metadata,All consumer member IDsm, Subscription info)  ‚Üí Assignment = All partitions 
        ‚Üí Leader sends assignment plan to Coordinator

    Seconds Consumer
        Consumer-2 starts  ‚Üí Connects to Broker  ‚Üí Find Coordinator (existing group coordinator) ‚Üí JoinGroup request sent ‚Üí Group Rebalance Triggered  
        ‚Üí Leader = Consumer-1 (already leader, Has Topic metadata, All consumer member IDs, Subscription info)  ‚Üí Leader Recomputes Assignment (Partitions shared across C1 & C2)  
        ‚Üí Leader sends new assignment plan to Coordinator  ‚Üí Coordinator sends assignment ‚Üí Consumer-1 & Consumer-2 


2. What causes consumer rebalancing?
    Consumer rebalancing in Kafka happens when the membership or assignment of a consumer group changes.
    Common causes include a consumer joining or leaving the group, a consumer failure or missed heartbeats, 
      topic partition changes, or configuration changes (like assignment strategy).
    When this happens, Kafka pauses consumption and redistributes partitions among active consumers.

3. Why are frequent rebalances dangerous?
    Frequent rebalances are dangerous because:
     - Consumption stops during rebalancing, directly impacting throughput
     - They increase consumer lag and end-to-end latency
     - They can cause duplicate processing or missed commits
     - They indicate an unstable consumer group, often due to bad configs or slow consumers

4. What is the role of poll() in Kafka consumers?
    while (running) {
        ConsumerRecords<String, String> records =
                consumer.poll(Duration.ofMillis(100));
        for (ConsumerRecord<String, String> record : records) {
            process(record);
        }
        consumer.commitSync(); // or async
    }
    - poll() is the core method that drives a Kafka consumer.
    - It fetches records from the broker and sends heartbeats to the group coordinator to 
            keep the consumer alive.
    - If poll() is not called within the configured interval, Kafka assumes the consumer 
        is dead and triggers a rebalance.
    Key configs tied to poll()
        max.poll.interval.ms ‚Üí max time between poll() calls
        max.poll.records ‚Üí how many records returned per poll()
        session.timeout.ms & heartbeat.interval.ms ‚Üí liveness checks
    In Spring Boot, you usually don‚Äôt call poll() yourself. Spring Kafka runs a background 
    consumer loop that continuously calls poll() for you.

5. What happens if poll() is delayed too long?
    If poll() is delayed too long:
        The consumer stops sending heartbeats
        The group coordinator marks it as dead
        A rebalance is triggered
        Its partitions are revoked and reassigned to other consumers
        The slow consumer may later rejoin, causing more rebalances and possible duplicates
        Note : If heartbeats are sent but poll() is not called for too long ‚Üí consumer is considered DEAD (kicked out of group).

6. How does max.poll.interval.ms affect consumers?
    - max.poll.interval.ms defines the maximum time a consumer can take to process records 
      between two poll() calls.
    - If processing takes longer than this value, Kafka assumes the consumer is stuck, 
      removes it from the group, and triggers a rebalance.

7. Difference between auto-commit and manual commit?
    - Auto-commit
        Kafka commits offsets automatically at intervals
        Simple to use, but can cause message loss or duplicates
        Less control over when a message is considered processed

    - Manual commit
        Application commits offsets explicitly after processing
        Gives full control and reliability
        Slightly more code, but safer for production

8. When can offsets be committed but processing fails?
    Offsets can be committed but processing can still fail in these cases:
    - Auto-commit enabled: Offsets are committed before processing completes, and the app crashes 
       during processing.
    - Manual commit done too early: The app commits offsets before writing to DB / downstream system, 
       then fails.
    - Async commit (commitAsync): Commit succeeds, but processing fails immediately after.
    - External system failure: Kafka offset is committed, but DB/API update fails.

9. How do you design idempotent consumers?
    To design idempotent consumers, make your processing safe to run more than once for the same message 
    (because Kafka is often at-least-once).

    Common patterns (used in production)
    1) Use a unique event id + dedup store
        Include eventId (UUID/orderId+version) in each message
        Before processing, check a table/cache: ‚Äúhave I processed this eventId?‚Äù
        If yes ‚Üí skip; if no ‚Üí process and record it

    2) Make the downstream write idempotent
        Use UPSERT / INSERT ‚Ä¶ ON CONFLICT DO NOTHING
        Use unique constraints on eventId or (businessKey, version)
        Avoid ‚Äúadd 10‚Äù updates; prefer ‚Äúset state to X‚Äù updates

    3) Transactional pattern (best)
        In one DB transaction:
        apply business update
        store eventId (or update a version)
        Commit Kafka offset after DB commit

    4) Use exactly-once tools when applicable
        Kafka Streams EOS / transactions can help for Kafka‚ÜíKafka pipelines, but DB still needs idempotency.

10.What consumer bugs have caused production incidents?
    Here are real consumer bugs that commonly cause production incidents, phrased the way interviewers like (symptom + root cause).
    - Rebalance storm: poll() blocked by long processing ‚Üí missed heartbeats / max.poll.interval.ms exceeded ‚Üí constant rebalances, no progress.
    - Duplicate processing: manual commit done after processing but downstream isn‚Äôt idempotent ‚Üí retries/rebalances reprocess messages and create duplicates.
    - Message loss: offsets committed too early (auto-commit or wrong manual commit timing) ‚Üí crash after commit ‚Üí records skipped forever.
    - Hot partition lag: bad keying (same key) ‚Üí one partition overloaded ‚Üí one consumer maxed out, group looks ‚Äúhealthy‚Äù but latency spikes.
    - Infinite retry loop: poison message keeps failing and is retried forever ‚Üí partition stuck, backlog grows.
    - OOM / GC pauses: fetching huge batches or large messages ‚Üí JVM GC stalls ‚Üí missed heartbeats ‚Üí rebalances + lag.
    - Threading mistake: using one KafkaConsumer across multiple threads ‚Üí unstable behavior, missed polls, exceptions, stalled consumption.
    - Wrong offset reset: misused auto.offset.reset (e.g., latest in a new group) ‚Üí starts after existing data ‚Üí ‚Äúmissing events‚Äù incident.
    - Bad error handling: swallowing exceptions and continuing commits ‚Üí silently drops failed records.
    - Under-provisioned consumers: too few consumers vs partitions or too slow processing ‚Üí lag grows until downstream SLAs break.


11. What happens when 200 records are polled and one fails?
    With @RetryableTopic (non-blocking retries), Spring Kafka typically does this:
    For a failed record on the main topic:
        1. Your listener throws InvalidItemException
        2. Spring Kafka‚Äôs retry infrastructure publishes that record to the retry topic (or DLT if retries are exhausted)
        3. After successfully publishing to retry/DLT, the framework commits/acknowledges the offset of the original record on the main topic
        This is important: otherwise the consumer would be stuck replaying the same failing record forever.
    So even though your code did not call ack.acknowledge() for the failed record, Spring‚Äôs error handling layer commits 
    it once it has handed it off to retry/DLT.

12. If a non-included exception is thrown (e.g., NullPointerException, RuntimeException, etc.) in the consumer
    What Spring Kafka does (with @RetryableTopic)
        It does not send the record to the retry topics.
        It publishes the record directly to the DLT (because it‚Äôs considered ‚Äúnon-retryable‚Äù for this listener).

    How commits work in that case
    Even though your code did not call ack.acknowledge() (because you threw an exception),
    Spring Kafka will typically:
        Publish the failed record to the DLT
        Commit the offset of the original record on the main topic partition (so it won‚Äôt be replayed on the main topic)

    So: DLT publish acts like a ‚Äúrecovery,‚Äù and then offset is advanced.

13. In a Spring Kafka consumer using @RetryableTopic, suppose a poll fetches 200 records. Out of these, 195 records are processed and 
    196th records throw a non-retryable exception (e.g., NullPointerException which is not included in retryable exceptions). 
    The system is configured with DltStrategy.FAIL_ON_ERROR. What happens if publishing those failed records to the 
    Dead Letter Topic (DLT) also fails? How does Kafka manage offsets in this scenario?

Answer:

    In Spring Kafka, offset commits are tied to successful processing or safe hand-off of records to retry or dead-letter infrastructure. 
    When using @RetryableTopic, retryable exceptions are routed to retry topics, while non-retryable exceptions are directly routed to 
    the Dead Letter Topic (DLT). If a record fails with a non-retryable exception, Spring Kafka attempts to publish that record to the DLT. 
    Only after successful publishing to the DLT does Spring Kafka commit the offset for the original record from the main topic. 
    This ensures that no message is lost and that Kafka guarantees at-least-once delivery semantics.

    However, when DltStrategy.FAIL_ON_ERROR is configured and the DLT publish itself fails (for example, if the DLT topic is unavailable, 
    broker is down, or authorization fails), Spring Kafka cannot safely move the record anywhere. In this case, the consumer 
    container throws an exception and stops processing. Since the record was neither processed successfully nor stored in
    a retry or DLT topic, the offset is not committed. When the consumer restarts or a rebalance occurs, 
    Kafka will redeliver all records starting from the last committed offset. This can result in reprocessing of multiple records from the batch.



14. In Apache Kafka, if a consumer is already part of a consumer group and has previously processed messages with committed offsets, 
but the configuration auto.offset.reset=earliest is set, will the consumer read all old messages again or start from the last committed 
offset? Explain the behavior with reasoning.

Answer

    If a Kafka consumer is part of an existing consumer group and that group already has committed offsets stored in Kafka, the consumer will 
    always resume reading from the last committed offset, regardless of the auto.offset.reset configuration. The auto.offset.reset property 
    is only used when there is no committed offset available for that consumer group and partition. This typically happens when a new consumer 
    group is created, offsets have expired due to retention policies, or a new partition is added and the group has never consumed from it before.
    In normal restart scenarios where offsets exist, Kafka ignores the reset policy and continues from where it left off to maintain data 
    processing continuity and prevent duplicate reads.

    For example, consider a topic where offsets range from 0 to 1000. If a consumer group has previously processed messages and committed offset 500,
    when the consumer restarts, it will begin consuming from offset 501 even if auto.offset.reset is set to earliest. 
    The consumer will not go back and read from offset 0 because Kafka already has a valid checkpoint. However, 
    if the consumer group is new and no offset exists, then auto.offset.reset=earliest would cause the consumer to start reading from offset 0. 
    This design ensures Kafka provides predictable and reliable message consumption while preventing accidental replay of 
    large amounts of historical data.

******************************************************************************************************************************

**** Kafka Consumer exactly-one semantic***
- Kafka itself does not provide true "exactly once" delivery to a consumer-only application; it provides "at least once" or "at most once".
- "Exactly once" semantics require the consumer's processing to be idempotent, and the consumer must commit offsets only after successful processing.
- Set consumer isolation level to read_committed if upstream producers use transactions.
- Use manual offset commit (disable auto-commit).
- Ensure your processing logic is idempotent to avoid side effects from possible duplicate deliveries.


    @Configuration
    @EnableKafka
    public class ExactlyOnceConsumerConfig {

        @Bean
        public ConsumerFactory<String, String> consumerFactory() {
            Map<String, Object> props = new HashMap<>();
            props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
            props.put(ConsumerConfig.GROUP_ID_CONFIG, "exactly-once-group");
            props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); // manual commit
            props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed"); // only if producer uses transactions
            props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
            props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);

            return new DefaultKafkaConsumerFactory<>(props);
        }
    }

    // Consumer

    @RetryableTopic(
        backoff = @Backoff(value = 3000L, multiplierExpression = "2"),
        attempts = "4", // N-1 Retry attempts
        topicSuffixingStrategy = TopicSuffixingStrategy.SUFFIX_WITH_INDEX_VALUE,
        autoCreateTopics = "false",
        dltTopicSuffix = "-dlt",
        dltStrategy = DltStrategy.FAIL_ON_ERROR,
        include = DelayException.class
    )
    @KafkaListener(
        topics = "${topic-name}",
        groupId = "${groupid}"
    )
    public void consume(
        @Payload String message,
        @Header(value = KafkaHeaders.OFFSET, required = false) Long offset,
        @Header(value = KafkaHeaders.RECEIVED_TOPIC, required = false) String topic,
        @Headers MessageHeaders headers,
        Acknowledgment ack
    ) {

        LOGGER.info("#### -> Received message {} from topic {} with offset {}", message, topic, offset);

        try {
            

            bigMacMessageProcessor.processMessage(message);

        } catch (final DelayException delayExp) {
            LOGGER.error("Retrying message due to DelayException: {}", delayExp.getMessage());
            throw delayExp;

        } catch (Exception ex) {
            LOGGER.error("Moving message to DLT due to exception: {}", ex.getMessage());
            throw new Exception(ex);
        }

        ack.acknowledge();
    }

********************************************************************************************************************

Consumer concurrency (4 threads in one process) = one Kafka consumer instance internally processing in parallel.

4 separate processes (each with 1 consumer) = four independent Kafka consumer instances working in parallel.

Kafka View: Group Membership & Partitions
    - Consumer Concurrency (Single Process, 4 Threads)
        A Kafka application runs one consumer instance inside a single process.
        This consumer polls records from its assigned partitions and pushes them into an internal work queue.

        Inside the same process, four worker threads pull records from that queue and process them in parallel.

        From Kafka‚Äôs perspective, the application is one group member, so partition assignment happens only once ‚Äî to that single consumer.

        Implication:
        Throughput can improve for processing, but fetch parallelism is limited to partitions assigned to that one consumer instance.
    
    - 4 Separate Processes (4 Consumers)
        A Kafka deployment runs 4 separate processes, each with one consumer instance.

        Each process joins the same consumer group, so Kafka sees 4 group members.
        Kafka distributes partitions across all four consumers automatically.

Fault Isolation
    - Consumer Concurrency (Single Process, 4 Threads)
        If the process crashes, the consumer and all 4 workers stop, and Kafka must rebalance partitions.

    - 4 Separate Processes (4 Consumers)
        If one process crashes, only that consumer is lost ‚Äî the other three continue processing without interruption.

********************************************************************************************************************
Kafka comsumer common properties


| Property Name           | Description                                                                                             |
| ----------------------- | ------------------------------------------------------------------------------------------------------- |
| bootstrap.servers       | Comma-separated list of Kafka broker addresses.                                                         |
| group.id                | Consumer group identifier.                                                                              |
| key.deserializer        | Deserializer class for message keys (e.g., `org.apache.kafka.common.serialization.StringDeserializer`). |
| value.deserializer      | Deserializer class for message values.                                                                  |
| enable.auto.commit      | If true, consumer‚Äôs offset is periodically committed automatically.                                     |
| auto.commit.interval.ms | Frequency (ms) of auto-commit if enabled.                                                               |
| auto.offset.reset       | What to do when there is no initial offset (earliest, latest, none).                                    |
| fetch.min.bytes         | Minimum amount of data the server should return for a fetch request.                                    |
| fetch.max.bytes         | Maximum amount of data the server should return for a fetch request.                                    |
| max.poll.records        | Maximum records returned in a single poll.                                                              |
| max.poll.interval.ms    | Maximum delay between invocations of poll().                                                            |
| session.timeout.ms      | Timeout used to detect consumer failures.                                                               |
| heartbeat.interval.ms   | Interval for sending heartbeats to the broker.                                                          |
| client.id               | Logical identifier for the consumer.                                                                    |
| security.protocol       | Security protocol (PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL).                                           |
| ssl.*                   | SSL configuration properties.                                                                           |
| sasl.*                  | SASL configuration properties.                                                                          |


*******************************************************************************************