üî• Kafka Consumer Scenarios

1.  Consumer lag keeps increasing but CPU and memory look normal. How do you debug this?
    When lag increases but CPU/memory look normal, assume the consumer is not actually making progress
    (stuck on I/O, blocked, rebalancing, or reading the wrong partitions). Debug in this order:
        1) Confirm where lag is coming from
            Check per-partition lag (is it one partition/hot key or all partitions?)
            Compare produce rate vs consume rate for the topic

        2) Check for rebalances (most common ‚Äúlooks normal‚Äù cause)
            Look for logs like: revoking partitions / re-joining group
            If rebalances are frequent, investigate:
            long processing delaying poll()
            max.poll.interval.ms too low
            GC pauses / network stalls causing missed heartbeats

        3) Verify the consumer is polling + committing
            Is poll() being called continuously?
            Are offsets actually advancing in the group?
            If offsets aren‚Äôt moving: you‚Äôre stuck in processing or blocked downstream

        4) Look for downstream bottlenecks (CPU can still be normal)
            Common hidden blockers:
            DB/API calls slow or throttled
            Connection pool exhausted
            Synchronous logging / remote calls
            Lock contention (threads waiting, not using CPU)

        5) Check partition assignment vs capacity
            Too few consumers vs partitions ‚Üí some consumers handle many partitions
            One ‚Äúhot partition‚Äù (skewed key) ‚Üí only one consumer falls behind

        6) Check fetch/processing settings
            max.poll.records too high ‚Üí long processing per poll (lag grows)
            fetch.min.bytes / fetch.max.wait.ms too aggressive ‚Üí slow fetch cadence
            Very small batches ‚Üí high overhead (but CPU may still look ‚Äúfine‚Äù)

        7) Look for poison messages / infinite retry
            Same offset repeatedly failing (exceptions + retries)
            Partition appears ‚Äústuck‚Äù on one record ‚Üí use DLQ / skip strategy

2.  After a deployment, consumers stop processing messages but offsets keep committing. What went wrong?
    This usually means the consumer is committing offsets without actually processing records.
    What likely went wrong:
        Auto-commit enabled ‚Üí offsets commit on a timer even if processing logic is broken
        Manual commit placed too early (before processing code)
        Exception swallowed after poll() ‚Üí code skips processing but still commits
        Processing thread not running after deploy (misconfigured thread pool / feature flag)
        Wrong deserializer / schema change ‚Üí records fail silently but commits continue

3.  Kafka consumers restart frequently during peak traffic. Why?
    Kafka consumers restarting frequently during peak traffic is usually due to missed heartbeats or 
    timeouts under load.
    Common reasons:
        Processing takes too long, so poll() isn‚Äôt called within max.poll.interval.ms
        GC pauses / memory pressure during high load cause heartbeat delays
        Downstream calls (DB/API) slow down and block consumer threads
        Too large batches (max.poll.records) increase processing time per poll
        Rebalance storms triggered by slow or unstable consumers

4.  One consumer instance is much slower than others in the same group. How do you investigate?
    If one consumer is much slower than others in the same group, I‚Äôd investigate in this order:
    Check partition assignment ‚Üí see if that consumer owns a hot partition (skewed key, uneven traffic)
    Compare per-partition lag ‚Üí confirm whether slowness is data-related or instance-specific
    Inspect processing path ‚Üí slow DB/API calls, locks, or retries on that instance
    Check GC / pauses ‚Üí GC logs, heap pressure, stop-the-world pauses
    Verify configs & version ‚Üí same code, same configs, same resources as others

5.  Consumers process messages twice after a restart. Is this expected? How do you prevent it?
    Yes ‚Äî this is expected in Kafka under the default at-least-once delivery model.
    Why it happens:
        The consumer processes messages
        It crashes or restarts before committing offsets
        On restart, Kafka resumes from the last committed offset
        Those messages are processed again
        So duplicates after restart are normal unless you design for them.
    How to prevent or handle it:
        Use manual commits after successful processing
        Make consumers idempotent (dedup by eventId, UPSERTs, unique keys)
        For Kafka‚ÜíKafka pipelines, use transactions / exactly-once semantics
        Avoid committing offsets before downstream writes

6.  A consumer crashes on a specific message and keeps reprocessing it forever. What do you do?
    This is a classic poison message scenario.

    What‚Äôs happening:
    The consumer fails on the same record, restarts, reads it again (offset not committed), 
    and crashes repeatedly.

    What to do:
        Identify the bad record (log topic/partition/offset)
        Skip or isolate it:
            Send it to a DLQ (Dead Letter Queue)
            Or manually seek past the offset after logging
        Add bounded retries (retry N times, then give up)
        Fix the root cause (bad data, schema mismatch, deserialization issue)

7.  Rebalancing happens too frequently and throughput collapses. What causes this?
    Frequent rebalancing with throughput collapse is usually caused by consumers 
    repeatedly leaving and rejoining the group.
    Common causes:
        Long processing time ‚Üí poll() not called within max.poll.interval.ms
        GC pauses / JVM stalls under load ‚Üí missed heartbeats
        Blocking downstream calls (DB/API) in the poll thread
        Too many consumer restarts (crash loops, bad liveness probes)
        Over-aggressive configs (session.timeout.ms, heartbeat.interval.ms)
        Eager rebalancing strategy instead of cooperative

8.  Consumers fall behind only at night. What could explain this?
    If consumers fall behind only at night, it‚Äôs usually due to environmental or workload 
    changes, not Kafka itself.
    Common explanations:
        Batch jobs / ETL / backups at night slowing DBs or APIs used by consumers
        Traffic spike (end-of-day processing, reports, settlements) increasing produce rate
        Fewer consumer instances (autoscaling scales down at night)
        Resource contention (shared infra, noisy neighbors)
        Network throttling or maintenance windows
        GC pressure due to larger messages or batch sizes at night
        
9.  One partition is always lagging while others are fine. Why?
    This almost always points to a hot partition problem.
    Why it happens:
        Skewed keys ‚Üí many messages use the same key, all routed to one partition
        Uneven traffic pattern ‚Üí one business entity (user/order/account) dominates
        Slow processing for that partition‚Äôs data (heavier DB calls, retries)
        Single-threaded consumption per partition ‚Üí it can‚Äôt be parallelized further

10. After scaling consumers, throughput doesn‚Äôt improve. Why?
    This usually means you‚Äôve hit a parallelism or bottleneck limit.
    Common reasons:
        Partitions ‚â§ consumers ‚Üí extra consumers sit idle (no more parallelism)
        Hot partition ‚Üí one partition dominates traffic; scaling consumers won‚Äôt help
        Downstream bottleneck ‚Üí DB/API is the real limiter
        Single-threaded processing per partition ‚Üí can‚Äôt parallelize within a partition
        Broker or network limits ‚Üí fetch/produce can‚Äôt go faster


üöÄ Kafka Producer Scenarios

1.  Producer logs show ‚Äúmessage sent successfully‚Äù but consumers don‚Äôt receive anything. How do you debug?
    Debug this like a checklist ‚Äî most ‚Äúsent but not consumed‚Äù issues are topic/offset/group/config problems, 
    not Kafka ‚Äúlosing‚Äù messages.
    1) Confirm the producer really wrote to the right place
    Verify topic name (env prefix, typo, wrong cluster/region)
    Check if records exist with: kafka-console-consumer --from-beginning (no group)

    2) Check consumer group offsets (most common)
    Consumer might be reading but from a different point:
    wrong group.id
    auto.offset.reset=latest + new group (skips old messages)
    offsets already at end
    Use kafka-consumer-groups --describe to see current offsets vs log end

    3) Check subscription & partition assignment
    Ensure consumer is subscribed to the same topic
    Confirm it owns partitions (no rebalancing loop)

    4) Check filtering / deserialization
    Message might be fetched but dropped:
    deserializer/schema mismatch
    app-level filtering (key/type mismatch)
    exceptions swallowed

    5) Check security / connectivity (if applicable)
    ACLs allow producer but not consumer
    SASL/SSL misconfig on consumer side

2.  Producer latency spikes suddenly with no code change. What could cause this?
    The time taken by Kafka producer to send a message and get acknowledgment from broker suddenly increases
    Sudden Kafka producer latency spikes with no code change are usually caused by infrastructure, 
        broker, or config-dependent runtime conditions:
        - Broker load / disk I/O slow (log flush, compaction, high IO wait) ‚Üí slower acks
        - ISR shrink / replica lag with acks=all ‚Üí producer waits longer
        - Leader elections / partition reassignments(moving to other broker) ‚Üí transient errors + retries
        - Network latency / packet loss / throttling between producer and brokers
        - Request queue saturation on brokers ‚Üí delayed processing/acks
        - GC pauses / CPU steal on producer JVM host ‚Üí delayed send thread
        - Increase in message size or throughput (same code, different traffic) ‚Üí batching/queueing delays
        - Batching effects (linger.ms, batch.size) ‚Üí queues fill, flush timing changes
        - Quota limits / throttling (broker/client quotas) ‚Üí forced delays

3.  Messages are duplicated after enabling retries. Why?
    Messages get duplicated after enabling retries because the producer may resend a message when it 
    doesn‚Äôt receive an acknowledgment‚Äîeven though the broker already wrote it.
     - Ack lost / leader change(after replicating to 50% replicas) ‚Üí producer retries
     - Broker writes the same record again ‚Üí duplicate messages
    Fix (one-liner):
     - Enable idempotence: enable.idempotence=true
     ‚Üí broker deduplicates retries

4.  Producers start timing out intermittently under load. What do you tune?
    Intermittent producer timeouts under load ‚Äî tune these (in order):
        Increase send capacity
            batch.size ‚Üë, linger.ms ‚Üë (better batching)
            buffer.memory ‚Üë (avoid local buffer exhaustion)
            Monitor max.block.ms / bufferpool-wait (timeouts often come from waiting for buffer)

        Reduce request pressure
            compression.type = lz4 or snappy
            max.in.flight.requests.per.connection (keep reasonable; with idempotence ‚â§ 5)

        Handle broker/ACK latency
            acks=all + slow replicas can cause timeouts ‚Üí check ISR, disk/network
            request.timeout.ms ‚Üë (if brokers are slow but healthy)
            delivery.timeout.ms ‚Üë (upper bound for retries + waiting)

        Retry/backoff correctly
            retries high (or default), retry.backoff.ms ‚Üë slightly to avoid retry storms

        Rule of thumb line:
        If timeouts happen only under load, it‚Äôs usually buffer pressure (client-side) or broker 
        ack latency (server-side)‚Äîtune accordingly.

5.  A producer crash causes message loss. How is that possible?
    A producer crash can still cause message loss when the message was never durably stored or 
    the app thought it sent but didn‚Äôt verify:
        - Async send without checking: app doesn‚Äôt get()/callback, process crashes before send completes.
        - Buffered in client: record is only in producer memory (buffer.memory/batch) and not yet sent.
        - acks=0 or acks=1 + leader dies: ack returned before replicas copied ‚Üí leader crash can lose it.
        - Retries disabled / low timeouts: transient error + crash before resend.
        - linger.ms/batching window: message waiting to batch when crash happens.
        - Not flushing on shutdown: no flush()/close() ‚Üí in-flight records dropped.

6.  Large messages cause producer OOM errors. How do you fix it?
    Large messages can blow up producer memory because they sit in client buffers, batches, 
    and sometimes compressed copies.
    Fixes (practical + interview-ready):
        - Reduce message size: send only needed fields, compress payload upstream, use efficient 
        formats (Avro/Protobuf), avoid huge JSON.
        - Store payload externally: put large blobs in S3/DB, send only a key + URL/id in Kafka.
        - Raise Kafka size limits (only if you must):
            Producer: max.request.size ‚Üë
            Broker: message.max.bytes ‚Üë (and topic-level if used)
            Consumer: fetch.max.bytes, max.partition.fetch.bytes ‚Üë (otherwise consumers break)
        - Prevent producer memory pressure:
            buffer.memory ‚Üë (more room)
            Reduce batching for big records: batch.size ‚Üì, linger.ms ‚Üì
            compression.type = lz4/snappy (often helps, but doesn‚Äôt fix truly huge records)

    Rule of thumb:
    Kafka is best for events, not big blobs‚Äîuse Kafka for references, not raw large payloads.

7.  Producer throughput drops after enabling compression. Why?
    Producer throughput can drop after enabling compression because compression is CPU-intensive 
    and shifts the bottleneck from network to the producer CPU.
        Why it happens:
            CPU overhead for compressing batches (especially gzip)
            Small batches (batch.size low / linger.ms low) ‚Üí poor compression efficiency
            High message rate ‚Üí producer threads spend more time compressing than sending
            Content already compressed (JSON with Base64, images) ‚Üí no size gain, only CPU cost
        What to tune:
            Use faster codecs: lz4 or snappy (avoid gzip unless bandwidth is the bottleneck)
            Increase batching: batch.size ‚Üë, linger.ms ‚Üë
            Scale producers or add threads (more producers, not more partitions per producer)
            Disable compression for already-compressed payloads

8.  Messages are unevenly distributed across partitions. What‚Äôs wrong?
    Uneven partition distribution is usually a keying/partitioner issue:
        Same or skewed key (e.g., userId=UNKNOWN, constant key) ‚Üí most records hash to 1‚Äì2 partitions.
        Null key + sticky partitioner ‚Üí producer tends to ‚Äústick‚Äù to one partition per batch; across time it balances, but can look uneven in short windows.
        Custom partitioner bug ‚Üí always returns same partition / bad modulo logic.
        Key has hotspots (few ‚Äúheavy‚Äù users/tenants) ‚Üí natural skew even with correct hashing.
        Too few partitions vs traffic pattern ‚Üí one partition becomes a hotspot.
    Fixes:
        Choose a high-cardinality key (or add a sharding suffix: key = userId + ":" + (hash(orderId)%N)).
        If ordering not required, consider no key (round-robin-ish over time) or a better key.
        Remove/fix the custom partitioner.
        Add partitions only if you can handle rebalancing and key remapping consequences.

9.  One producer floods Kafka and impacts all other services. How do you isolate it?
    Isolate the ‚Äúnoisy‚Äù producer by putting hard limits around it at multiple layers:
    - Separate topic / cluster (best isolation)
        Put that workload on its own topics (and ideally its own cluster if it‚Äôs truly abusive).
    - Quota it on the broker
        Use Kafka client quotas (per client.id / user / IP) to cap produce bandwidth/requests so it can‚Äôt starve others.
    - Dedicated resources
        Separate brokers for that workload (broker rack/tagging, separate cluster)
        Dedicated partitions / replication factor tuned for it
    - Backpressure at the producer
        Lower max.in.flight, limit concurrency, set sane buffer.memory + max.block.ms, rate-limit in app.
    - Multi-tenant hygiene
        Use separate service accounts / credentials and ACLs, so quotas and monitoring can target that one producer cleanly.

10. Producer sends data faster than consumers can handle. How do you handle backpressure?
    Handle backpressure by slowing producers or absorbing load safely:
        - Let Kafka buffer, but cap it
            Consumers scale out (more instances) and/or scale up partitions; add disk/IO on brokers.
        - Rate-limit at producer
            Token bucket / QPS limit, or block on send() by keeping buffer.memory bounded + low max.block.ms 
            (forces the app to slow down instead of OOM).
        - Shed or degrade non-critical traffic
            Drop/skip low-priority events, sample logs/metrics events, or route to a ‚Äúdead letter / overflow‚Äù topic.
        - Use retry + DLQ with limits
            Don‚Äôt retry forever; cap attempts and push failures to DLQ.
        - Consumer-side tuning
            Increase parallelism (more consumers/partitions), optimize processing, and tune max.poll.records / batching.

‚öñÔ∏è Offset & Delivery Semantics Scenarios

1. Offsets are committed but database records are missing. How did this happen?
    The consumer can commit the offset even though the DB insert didn‚Äôt actually make it.
    Common real causes:
    - Committed before processing finishes
        If a Kafka consumer commits the offset before successfully writing the record to the database, Kafka assumes the message is 
        processed and will not resend it. If the application crashes or the DB insert fails after the commit, the record is permanently 
        lost in the database but marked as processed in Kafka. This commonly happens when auto-commit is enabled or manual commit is 
        placed before processing completes. The safe pattern is to process ‚Üí write to DB ‚Üí then commit offset. For stronger guarantees, 
        use idempotent DB writes or transactional patterns.
    
    - Async DB write
        If you do async DB writes and commit offsets immediately, you risk data loss.
        Example: You fire an async DB insert, then commit the Kafka offset, and the app crashes before the async write finishes.
            for (record : poll()) {
                asyncDb.insertOrder(record.value());  // fire-and-forget
                consumer.commitSync();                // commit offset immediately
            }
    
    
    - Rebalance during processing
        During a rebalance, a consumer can lose partition ownership while it‚Äôs still processing records.
        If your app commits offsets after losing the partition, you may commit work that was never fully processed.
        Example: You poll records, rebalance happens during processing, but commitSync() still runs.
        Kafka assumes records are done, but another consumer may reprocess or skip them ‚Üí duplicates or data loss.
            records = consumer.poll(...);          // got P0: [100,101,102]
            process(records);                      // takes time...
            consumer.commitSync();                 // ‚ùå may run after rebalance

*****************************************************************************************************************************
2. You need exactly-once processing but still see duplicates. Why?
    Kafka "exactly-once" processing requires correct configuration and code on both producer and consumer sides.

    Duplicates can still occur if:
        Producer is not fully idempotent or not using transactions
            ‚Üí Retries or duplicate sends can produce multiple copies of the same event into Kafka.

        Consumer commits offset before DB write is confirmed
            ‚Üí If the consumer crashes after committing but before the DB write completes, the event is lost or inconsistently processed.

        Consumer logic is not idempotent
            ‚Üí Reprocessing the same message (after retry or restart) causes duplicate side effects like duplicate DB rows.

        There are failures or rebalances between DB write and offset commit
            ‚Üí The message may be processed again after rebalance/restart, leading to duplicate processing.

        Not using isolation.level=read_committed on consumer
            ‚Üí Consumer may read aborted transactional messages, causing inconsistent or duplicate downstream processing.

        Outbox pattern or transactional guarantees are not implemented end-to-end
            ‚Üí Kafka may be exactly-once internally, but duplicates or inconsistencies still occur across Kafka ‚Üî DB ‚Üî external systems.
    
        Checklist:

        Producer:
            enable.idempotence=true
            Use transactions if writing to Kafka and DB together.

        Consumer:
            isolation.level=read_committed
            Commit offset only after DB write is successful.
            Use manual offset commit, not auto-commit.

        Processing:
            Ensure DB writes are idempotent (e.g., upsert, deduplication).
            Handle errors and retries carefully.

        System:
            Avoid consumer rebalances during processing

*****************************************************************************************************************************

3. Business asks to replay last 3 days of data without breaking systems. How do you do it?
    Replay into a new consumer group
        Create group.id=replay-2026-01-27 so existing consumers aren‚Äôt affected.

    Limit the window to last 3 days
        Seek by timestamp: use offsetsForTimes(now-3days) (or kafka-consumer-groups --reset-offsets 
        --by-duration P3D --to-earliest/--to-latest as needed).

    Throttle + control blast radius
        Rate-limit the replay consumer (sleep/token bucket), cap concurrency, and keep batch sizes small.

    Make processing idempotent
        Ensure downstream writes are safe: upsert/unique keys, dedupe by eventId, or exactly-once patterns.

    Isolate outputs
        Optionally write replay results to separate topics / DB tables first, then merge.

    Monitor + rollback plan
        Track lag, error rate, and downstream load; be able to pause/stop quickly.

*****************************************************************************************************************************

4. A consumer accidentally resets offsets to earliest in production. What do you do next?
    1. Identify the impact:
        All messages from the earliest offset will be reprocessed, causing possible duplicates or side effects.
    2. Stop the consumer immediately to prevent further processing.
    3. Determine the correct offset to resume from (last successfully processed message).
    4. Use Kafka tools to reset the consumer group offset to the correct value.
    5. Restart the consumer.

    Steps:
        1. Stop the Consumer Application
           Prevent further duplicate processing.

        2. Find the Correct Offset
            Check logs, DB, or audit tables to determine the last successfully processed offset for each partition.

        3. Reset Offsets to the Correct Value

            Use kafka-consumer-groups.sh to set offsets:

            kafka-consumer-groups.sh --bootstrap-server <broker> \
            --group <consumer-group> \
            --topic <topic>:<partition>,<offset> \
            --reset-offsets --execute


            Example for partition 0, offset 12345:

            kafka-consumer-groups.sh --bootstrap-server broker:9092 \
            --group my-group \
            --topic my-topic:0,12345 \
            --reset-offsets --execute


        4. Restart the Consumer
            Resume processing from the correct offset.

*****************************************************************************************************************************

5. Duplicate messages corrupt downstream systems. How do you redesign?
    Simple Real-World Story ‚Äî Online Order Payment

    We had a system where Kafka sent Payment Success events.
    Sometimes due to retries, the same payment event came twice ‚Üí caused duplicate ledger entries and duplicate emails.

    So we redesigned the system to make duplicate processing safe.
    - Make consumers idempotent (primary fix)
        Each payment event had a unique paymentEventId.
        If same event came again, DB ignored it or updated same row ‚Üí no duplicate money or order change.

    - Transactional boundaries
        If writing to DB: DB transaction = (apply change + store eventId/offset) atomically
        We updated order status and saved paymentEventId in same DB transaction.
        If service crashed, retry was safe because either both saved or both failed.

    - Handle ordering + concurrency
        All events for same order went to same Kafka partition.
        This prevented wrong sequences like Refund processed before Payment.

    - Producer-side safety
        Payment service used idempotent Kafka producer.
        This reduced duplicates created during retries or network failures.

    - Operational guardrails
        We added DLQ for bad payment messages and retry with delay.
        If duplicate detection increased suddenly, alerts notified us.

*****************************************************************************************************************************

6. Offset commits fail intermittently. What are the consequences?
    1Ô∏è‚É£ Duplicates / Reprocessing
    What happens: After restart or rebalance, consumer re-reads old messages.
        Example:
        Consumer processes order payment event O-9
        DB updated ‚Üí status = PAID
        Offset commit fails
        Service restarts ‚Üí Kafka sends same message again
        Order gets processed again (duplicate processing attempt)

    2Ô∏è‚É£ Higher Lag & Slower Recovery
    What happens: Consumer is processing messages but not committing ‚Üí lag keeps growing.
    Example:
        Consumer processes messages from offset 100 ‚Üí 200
        Commit keeps failing ‚Üí Kafka still thinks you are at offset 100
        Lag dashboards show you are ‚Äúbehind‚Äù
        After restart ‚Üí consumer must reprocess 100‚Äì200 again ‚Üí slower recovery

    3Ô∏è‚É£ Possible Downstream Side-Effects (if not idempotent)
    What happens: Reprocessing causes real-world duplicate actions.
    Example:
        Message = ‚ÄúSend order confirmation email‚Äù
        Email sent successfully
        Offset commit fails
        Message reprocessed ‚Üí customer gets 2 emails
        OR
        Message = ‚ÄúInsert payment record‚Äù
        Without unique key ‚Üí duplicate payment row created

    4Ô∏è‚É£ Rebalance Instability
    What happens: Commit failures often correlate with broker/coordinator/network issues ‚Üí group rebalances more.
    Example:
        Network jitter ‚Üí commit requests timeout
        Consumer heartbeat gets delayed
        Broker triggers rebalance
        New consumer instance starts from older committed offset ‚Üí repeats work

    5Ô∏è‚É£ Rare Case: App Stops on Commit Failure
    What happens: If app treats commit failure as fatal ‚Üí processing halts (but data still safe in Kafka).
    Example:
        App throws fatal exception when commit fails
        Consumer container stops
        No messages processed until restart
        But Kafka still has all data ‚Üí no data loss, just downtime

*****************************************************************************************************************************

7. How do you safely pause consumption during incidents?
    Stop the consumer group (hard stop)
        Scale the consumer deployment to 0 / stop the service. No polling, no commits.

    Pause partitions (soft stop, keep membership)
        consumer.pause(assignedPartitions) / in Spring Kafka use pause the listener container. 
        Keeps the consumer alive but stops fetching new records.

    Backpressure throttling (degrade mode)
        Reduce concurrency, lower max.poll.records, add rate-limit/sleeps so downstream isn‚Äôt overwhelmed.

    Protect downstream
        Temporarily disable side effects (emails/external calls) or route to DLQ/holding topic.

*****************************************************************************************************************************

8. A rollback requires reprocessing messages already consumed. How do you handle it?
    Create a dedicated ‚Äúrollback replay‚Äù consumer group (don‚Äôt disturb prod group).
    
    Reset that group‚Äôs offsets back to the rollback point:
        by timestamp (best) or exact offsets.

    Reprocess with idempotency:
        use eventId + DB upsert/unique constraint (or a dedupe table) so replay won‚Äôt corrupt data.

    Control blast radius:
        throttle concurrency / max.poll.records, and optionally write results to a staging topic/table first.

    Resume prod safely:
        once data is corrected, switch traffic/consumers back and monitor.

*****************************************************************************************************************************

9. Messages arrive out of order. Why?
    Different partitions: same ‚Äúbusiness entity‚Äù events go to different partitions (wrong/nullable key) ‚Üí 
        order not guaranteed.
    Multiple producers: two producers write related events ‚Üí Kafka can‚Äôt enforce cross-producer ordering.
    Retries without idempotence: retries can reinsert and appear out of order (especially with max.in.flight > 1).
    Consumer-side parallelism: processing records concurrently (threads/reactive) finishes in a different 
        order than polled.
    Rebalances / restarts: reprocessing + async processing can make observed order look wrong.

*****************************************************************************************************************************

10. Consumers reprocess messages after scaling down. Is this expected?
    Why it happens:
        When you scale down, consumers leave the group.
        Kafka triggers a rebalance.
        Partitions move to remaining consumers.
        New consumers resume from the last committed offset, not the last processed record.
        If some records were processed but offsets weren‚Äôt committed yet ‚Üí they get reprocessed.

    Common reasons it shows up more during scale-down:
        In-flight messages when the consumer shuts down
        Async processing with late commits
        Auto-commit interval not reached yet

üß± Topic & Partition Design Scenarios

1. Topic was created with too few partitions and now can‚Äôt keep up. What are your options?
    Increase partitions (most common)
        kafka-topics --alter --topic X --partitions N
        Pros: more parallelism (more consumers/throughput)
        Cons: order per key changes, key‚Üípartition mapping changes, can increase rebalancing

    Scale consumers + tune processing
        Works only if you already have enough partitions (max parallelism = partitions)
        Optimize handler, batch processing, async IO, max.poll.records, etc.

    Split the topic
        Create X.v2 (or shard topics like X-0..X-7) and route producers by key/tenant
        Pros: clean sharding + better isolation
        Cons: app changes, migration complexity

    Add broker capacity
        More brokers / faster disks / better network if brokers are saturated

    Reduce data volume
        Compression, smaller payloads, drop non-critical events, sampling

2. One partition becomes a hotspot. How do you fix it without downtime?
    Fix a hotspot without downtime by reducing skew and moving load while keeping consumers running:

    Confirm it‚Äôs key skew
        Check if most records share the same/low-cardinality key (or null key).

    Add partitions + rolling update (no downtime)
        Increase partitions for the topic. Consumers will rebalance, but service stays up.

    Change partitioning to spread the hot key
        Add a shard suffix: key = userId + ":" + (hash(orderId) % N)
        Or custom partitioner that splits only hot keys
        Do this via a rolling deploy of producers.

    Split traffic gradually
        Create a new topic topic.v2 with more partitions and dual-write (or route a % of keys) ‚Üí then cut over consumers.

    Short-term mitigations
        Throttle that producer, compress, batch more, or scale brokers if IO-bound.

3. Increasing partitions breaks message ordering. Why?
    Partition is chosen by: hash(key) % numPartitions
    When numPartitions increases, the modulo result changes
    So the same key can start landing on a different partition
    Then events for that key are split across partitions ‚Üí no global ordering for that key anymore


5. A topic grows faster than expected and disk fills up. What do you do?
    Immediate containment
        Stop/reduce the producer (feature flag / rate-limit) to stop the bleed.
        Add disk/capacity fast: expand volume / add brokers (if you can quickly).
        Lower retention temporarily:
            reduce retention.ms / retention.bytes
            if safe, use cleanup.policy=delete (not compact-only)
        Run preferred leader / rebalancing only if needed (avoid making it worse during crisis).

    Find the cause
        Which producer/app is spiking? (client.id, ACL user, metrics)
        Is it larger messages, higher rate, or retention misconfig?
        Any stuck consumers causing retries/dup publishes upstream?

6. Retention cleanup causes performance degradation. Why?
    Segment deletions trigger lots of disk IO (delete + metadata updates).
    Log compaction (if enabled) scans and rewrites segments ‚Üí very IO/CPU intensive.
    Frequent cleanup (small segment.ms / aggressive retention.ms) runs too often.
    Slow disks (HDD / saturated SSD) amplify the impact.
    Large partitions mean bigger segments to scan/delete.

7. Compacted topic loses data unexpectedly. How?
    Only the latest value per key is kept
        Older records with the same key are deleted during compaction.

    Tombstones (null values)
        Producing key + null value marks the key for deletion ‚Üí after delete.retention.ms, the key disappears completely.

    Misunderstood retention vs compaction
        If cleanup.policy=compact,delete, old segments can still be deleted by time/size retention.

    Wrong / missing keys
        Records with null keys are not compacted (can be deleted by retention), or changing keys makes data look ‚Äúlost‚Äù.

    Aggressive compaction settings
        Small segment.ms/bytes or frequent cleaner runs ‚Üí faster removal than expected.

8. Partition reassignment causes lag spikes. Why?
    Partition reassignment causes lag spikes because it temporarily steals resources and pauses normal flow.
    Why it happens:
        Data copying: replicas are moved between brokers ‚Üí heavy disk + network IO.
        Throttling kicks in: Kafka slows replication to protect the cluster, delaying fetch/produce.
        Leader movement: leadership changes force consumers to reconnect and catch up.
        Cache cold starts: new leaders don‚Äôt have data in page cache ‚Üí slower reads.
        Extra coordination: metadata updates and ISR changes add overhead.

9. Multi-tenant topics cause noisy-neighbor issues. How do you isolate?

10. What happens if partition leaders move frequently?
    Produce/consume latency spikes (clients must discover new leader + retry).
    More request errors/timeouts (NotLeaderForPartition, LeaderNotAvailable) ‚Üí extra retries.
    Consumer lag increases (fetch pauses during leader change, then catch-up).
    Higher broker load (metadata updates, controller work, replica sync/ISR churn).
    Risk of reduced durability if ISR shrinks often (replicas falling out of sync).

üß¨ Schema & Data Evolution Scenarios

1. Producer deploys a schema change and consumers crash. What went wrong?
2. Old consumers break after a new field is added. Why?
3. Schema Registry is down ‚Äî what happens?
4. Different teams evolve schemas independently. How do you prevent outages?
5. A backward-compatible change still causes issues. Why?
6. How do you safely remove a field from events?
7. A schema change corrupts downstream data. How do you recover?
8. Schema compatibility rules are bypassed in production. What‚Äôs the impact?
9. Replay of old events fails due to schema mismatch. Why?
10. How do you test schema changes before production?

üß® Failure & Reliability Scenarios

Kafka broker goes down during peak traffic. What happens?
ISR shrinks frequently. What does that indicate?
Disk usage grows faster than expected. Why?
Network latency spikes affect Kafka cluster. What breaks first?
One broker is much slower than others. Why?
Zookeeper instability causes Kafka issues. How?
Kafka cluster survives but clients start failing. Why?
GC pauses on brokers cause lag. How do you detect?
Messages pile up but brokers look healthy. What do you check?
What failure modes can cause silent data loss?

‚öôÔ∏è Performance & Tuning Scenarios

Throughput suddenly drops by 50% after a config change. How do you debug?
Consumer fetch requests are slow. Why?
High CPU on brokers but low network usage. Why?
Compression improves throughput but increases latency. Why?
Kafka performs well in test but poorly in prod. Why?
Increasing batch size makes latency worse. Why?
Brokers run out of file handles. What caused it?
GC tuning improves throughput but causes latency spikes. Why?
How do you identify the true bottleneck ‚Äî producer, broker, or consumer?
What tuning mistakes commonly cause outages?

üåç Kafka in Distributed Systems Scenarios

A service publishes events but downstream state is inconsistent. Why?
Kafka is used as a request-response system and starts failing. Why?
A single event triggers a cascade of failures. How do you prevent this?
Event-driven workflow partially completes and gets stuck. Why?
Multiple services consume the same topic but interpret data differently. How do you fix this?
Replay causes downstream systems to double charge customers. What went wrong?
Event ordering across services is broken. Why?
Kafka becomes a shared blast radius. How do you redesign?
One bad consumer slows the entire ecosystem. How?
What Kafka anti-patterns have you seen in microservices?

üîê Security & Compliance Scenarios

Unauthorized consumer reads sensitive data. How did this happen?
ACL misconfiguration blocks production traffic. How do you recover?
Certificates expire and Kafka clients start failing. What breaks?
A team bypasses security using shared credentials. How do you prevent this?
Audit team asks who accessed which topics. How do you answer?
Kafka credentials leak. What‚Äôs your immediate action?
TLS handshake failures happen intermittently. Why?
IAM-based auth works in one region but not another. Why?
Security changes break older consumers. Why?
How do you rotate secrets without downtime?

üß† Senior / Architect-Level Scenarios
Kafka is used everywhere and becomes hard to govern. What do you change?
Business wants Kafka for everything. When do you say no?
Kafka cluster cost keeps increasing. How do you optimize?
One team‚Äôs design causes repeated Kafka incidents. How do you fix culturally and technically?
You need cross-region disaster recovery for Kafka. How do you design it?
A migration from MQ to Kafka causes message loss. Why?
You inherit a badly designed Kafka platform. Where do you start?
What Kafka decision caused the biggest outage you‚Äôve seen?
What would you redesign if starting Kafka from scratch?
What Kafka failure changed how you design systems?