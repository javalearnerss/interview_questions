üî• Kafka Consumer Scenarios

1.  Consumer lag keeps increasing but CPU and memory look normal. How do you debug this?
    When lag increases but CPU/memory look normal, assume the consumer is not actually making progress
    (stuck on I/O, blocked, rebalancing, or reading the wrong partitions). Debug in this order:
        1) Confirm where lag is coming from
            Check per-partition lag (is it one partition/hot key or all partitions?)
            Compare produce rate vs consume rate for the topic

        2) Check for rebalances (most common ‚Äúlooks normal‚Äù cause)
            Look for logs like: revoking partitions / re-joining group
            If rebalances are frequent, investigate:
            long processing delaying poll()
            max.poll.interval.ms too low
            GC pauses / network stalls causing missed heartbeats

        3) Verify the consumer is polling + committing
            Is poll() being called continuously?
            Are offsets actually advancing in the group?
            If offsets aren‚Äôt moving: you‚Äôre stuck in processing or blocked downstream

        4) Look for downstream bottlenecks (CPU can still be normal)
            Common hidden blockers:
            DB/API calls slow or throttled
            Connection pool exhausted
            Synchronous logging / remote calls
            Lock contention (threads waiting, not using CPU)

        5) Check partition assignment vs capacity
            Too few consumers vs partitions ‚Üí some consumers handle many partitions
            One ‚Äúhot partition‚Äù (skewed key) ‚Üí only one consumer falls behind

        6) Check fetch/processing settings
            max.poll.records too high ‚Üí long processing per poll (lag grows)
            fetch.min.bytes / fetch.max.wait.ms too aggressive ‚Üí slow fetch cadence
            Very small batches ‚Üí high overhead (but CPU may still look ‚Äúfine‚Äù)

        7) Look for poison messages / infinite retry
            Same offset repeatedly failing (exceptions + retries)
            Partition appears ‚Äústuck‚Äù on one record ‚Üí use DLQ / skip strategy

2.  After a deployment, consumers stop processing messages but offsets keep committing. What went wrong?
    This usually means the consumer is committing offsets without actually processing records.
    What likely went wrong:
        Auto-commit enabled ‚Üí offsets commit on a timer even if processing logic is broken
        Manual commit placed too early (before processing code)
        Exception swallowed after poll() ‚Üí code skips processing but still commits
        Processing thread not running after deploy (misconfigured thread pool / feature flag)
        Wrong deserializer / schema change ‚Üí records fail silently but commits continue

3.  Kafka consumers restart frequently during peak traffic. Why?
    Kafka consumers restarting frequently during peak traffic is usually due to missed heartbeats or 
    timeouts under load.
    Common reasons:
        Processing takes too long, so poll() isn‚Äôt called within max.poll.interval.ms
        GC pauses / memory pressure during high load cause heartbeat delays
        Downstream calls (DB/API) slow down and block consumer threads
        Too large batches (max.poll.records) increase processing time per poll
        Rebalance storms triggered by slow or unstable consumers

4.  One consumer instance is much slower than others in the same group. How do you investigate?
    If one consumer is much slower than others in the same group, I‚Äôd investigate in this order:
    Check partition assignment ‚Üí see if that consumer owns a hot partition (skewed key, uneven traffic)
    Compare per-partition lag ‚Üí confirm whether slowness is data-related or instance-specific
    Inspect processing path ‚Üí slow DB/API calls, locks, or retries on that instance
    Check GC / pauses ‚Üí GC logs, heap pressure, stop-the-world pauses
    Verify configs & version ‚Üí same code, same configs, same resources as others

5.  Consumers process messages twice after a restart. Is this expected? How do you prevent it?
    Yes ‚Äî this is expected in Kafka under the default at-least-once delivery model.
    Why it happens:
        The consumer processes messages
        It crashes or restarts before committing offsets
        On restart, Kafka resumes from the last committed offset
        Those messages are processed again
        So duplicates after restart are normal unless you design for them.
    How to prevent or handle it:
        Use manual commits after successful processing
        Make consumers idempotent (dedup by eventId, UPSERTs, unique keys)
        For Kafka‚ÜíKafka pipelines, use transactions / exactly-once semantics
        Avoid committing offsets before downstream writes

6.  A consumer crashes on a specific message and keeps reprocessing it forever. What do you do?
    This is a classic poison message scenario.

    What‚Äôs happening:
    The consumer fails on the same record, restarts, reads it again (offset not committed), 
    and crashes repeatedly.

    What to do:
        Identify the bad record (log topic/partition/offset)
        Skip or isolate it:
            Send it to a DLQ (Dead Letter Queue)
            Or manually seek past the offset after logging
        Add bounded retries (retry N times, then give up)
        Fix the root cause (bad data, schema mismatch, deserialization issue)

7.  Rebalancing happens too frequently and throughput collapses. What causes this?
    Frequent rebalancing with throughput collapse is usually caused by consumers 
    repeatedly leaving and rejoining the group.
    Common causes:
        Long processing time ‚Üí poll() not called within max.poll.interval.ms
        GC pauses / JVM stalls under load ‚Üí missed heartbeats
        Blocking downstream calls (DB/API) in the poll thread
        Too many consumer restarts (crash loops, bad liveness probes)
        Over-aggressive configs (session.timeout.ms, heartbeat.interval.ms)
        Eager rebalancing strategy instead of cooperative

8.  Consumers fall behind only at night. What could explain this?
    If consumers fall behind only at night, it‚Äôs usually due to environmental or workload 
    changes, not Kafka itself.
    Common explanations:
        Batch jobs / ETL / backups at night slowing DBs or APIs used by consumers
        Traffic spike (end-of-day processing, reports, settlements) increasing produce rate
        Fewer consumer instances (autoscaling scales down at night)
        Resource contention (shared infra, noisy neighbors)
        Network throttling or maintenance windows
        GC pressure due to larger messages or batch sizes at night
        
9.  One partition is always lagging while others are fine. Why?
    This almost always points to a hot partition problem.
    Why it happens:
        Skewed keys ‚Üí many messages use the same key, all routed to one partition
        Uneven traffic pattern ‚Üí one business entity (user/order/account) dominates
        Slow processing for that partition‚Äôs data (heavier DB calls, retries)
        Single-threaded consumption per partition ‚Üí it can‚Äôt be parallelized further

10. After scaling consumers, throughput doesn‚Äôt improve. Why?
    This usually means you‚Äôve hit a parallelism or bottleneck limit.
    Common reasons:
        Partitions ‚â§ consumers ‚Üí extra consumers sit idle (no more parallelism)
        Hot partition ‚Üí one partition dominates traffic; scaling consumers won‚Äôt help
        Downstream bottleneck ‚Üí DB/API is the real limiter
        Single-threaded processing per partition ‚Üí can‚Äôt parallelize within a partition
        Broker or network limits ‚Üí fetch/produce can‚Äôt go faster


üöÄ Kafka Producer Scenarios

1.  Producer logs show ‚Äúmessage sent successfully‚Äù but consumers don‚Äôt receive anything. How do you debug?
    Debug this like a checklist ‚Äî most ‚Äúsent but not consumed‚Äù issues are topic/offset/group/config problems, 
    not Kafka ‚Äúlosing‚Äù messages.
    1) Confirm the producer really wrote to the right place
    Verify topic name (env prefix, typo, wrong cluster/region)
    Check if records exist with: kafka-console-consumer --from-beginning (no group)

    2) Check consumer group offsets (most common)
    Consumer might be reading but from a different point:
    wrong group.id
    auto.offset.reset=latest + new group (skips old messages)
    offsets already at end
    Use kafka-consumer-groups --describe to see current offsets vs log end

    3) Check subscription & partition assignment
    Ensure consumer is subscribed to the same topic
    Confirm it owns partitions (no rebalancing loop)

    4) Check filtering / deserialization
    Message might be fetched but dropped:
    deserializer/schema mismatch
    app-level filtering (key/type mismatch)
    exceptions swallowed

    5) Check security / connectivity (if applicable)
    ACLs allow producer but not consumer
    SASL/SSL misconfig on consumer side

2.  Producer latency spikes suddenly with no code change. What could cause this?
    - Sudden Kafka producer latency spikes with no code change are usually caused by infrastructure, 
        broker, or config-dependent runtime conditions:
    - Broker load / disk I/O slow (log flush, compaction, high IO wait) ‚Üí slower acks
    - ISR shrink / replica lag with acks=all ‚Üí producer waits longer
    - Leader elections / partition reassignments ‚Üí transient errors + retries
    - Network latency / packet loss / throttling between producer and brokers
    - Request queue saturation on brokers ‚Üí delayed processing/acks
    - GC pauses / CPU steal on producer JVM host ‚Üí delayed send thread
    - Increase in message size or throughput (same code, different traffic) ‚Üí batching/queueing delays
    - Batching effects (linger.ms, batch.size) ‚Üí queues fill, flush timing changes
    - Quota limits / throttling (broker/client quotas) ‚Üí forced delays

3.  Messages are duplicated after enabling retries. Why?
    Messages get duplicated after enabling retries because the producer may resend a message when it 
    doesn‚Äôt receive an acknowledgment‚Äîeven though the broker already wrote it.
     - Ack lost / leader change ‚Üí producer retries
     - Broker writes the same record again ‚Üí duplicate messages
    Fix (one-liner):
     - Enable idempotence: enable.idempotence=true
     ‚Üí broker deduplicates retries

4.  Producers start timing out intermittently under load. What do you tune?
    Intermittent producer timeouts under load ‚Äî tune these (in order):
        Increase send capacity
            batch.size ‚Üë, linger.ms ‚Üë (better batching)
            buffer.memory ‚Üë (avoid local buffer exhaustion)
            Monitor max.block.ms / bufferpool-wait (timeouts often come from waiting for buffer)

        Reduce request pressure
            compression.type = lz4 or snappy
            max.in.flight.requests.per.connection (keep reasonable; with idempotence ‚â§ 5)

        Handle broker/ACK latency
            acks=all + slow replicas can cause timeouts ‚Üí check ISR, disk/network
            request.timeout.ms ‚Üë (if brokers are slow but healthy)
            delivery.timeout.ms ‚Üë (upper bound for retries + waiting)

        Retry/backoff correctly
            retries high (or default), retry.backoff.ms ‚Üë slightly to avoid retry storms

        Rule of thumb line:
        If timeouts happen only under load, it‚Äôs usually buffer pressure (client-side) or broker 
        ack latency (server-side)‚Äîtune accordingly.

5.  A producer crash causes message loss. How is that possible?
    A producer crash can still cause message loss when the message was never durably stored or 
    the app thought it sent but didn‚Äôt verify:
        - Async send without checking: app doesn‚Äôt get()/callback, process crashes before send completes.
        - Buffered in client: record is only in producer memory (buffer.memory/batch) and not yet sent.
        - acks=0 or acks=1 + leader dies: ack returned before replicas copied ‚Üí leader crash can lose it.
        - Retries disabled / low timeouts: transient error + crash before resend.
        - linger.ms/batching window: message waiting to batch when crash happens.
        - Not flushing on shutdown: no flush()/close() ‚Üí in-flight records dropped.

6.  Large messages cause producer OOM errors. How do you fix it?
    Large messages can blow up producer memory because they sit in client buffers, batches, 
    and sometimes compressed copies.
    Fixes (practical + interview-ready):
        - Reduce message size: send only needed fields, compress payload upstream, use efficient 
        formats (Avro/Protobuf), avoid huge JSON.
        - Store payload externally: put large blobs in S3/DB, send only a key + URL/id in Kafka.
        - Raise Kafka size limits (only if you must):
            Producer: max.request.size ‚Üë
            Broker: message.max.bytes ‚Üë (and topic-level if used)
            Consumer: fetch.max.bytes, max.partition.fetch.bytes ‚Üë (otherwise consumers break)
        - Prevent producer memory pressure:
            buffer.memory ‚Üë (more room)
            Reduce batching for big records: batch.size ‚Üì, linger.ms ‚Üì
            compression.type = lz4/snappy (often helps, but doesn‚Äôt fix truly huge records)

    Rule of thumb:
    Kafka is best for events, not big blobs‚Äîuse Kafka for references, not raw large payloads.

7.  Producer throughput drops after enabling compression. Why?
    Producer throughput can drop after enabling compression because compression is CPU-intensive 
    and shifts the bottleneck from network to the producer CPU.
        Why it happens:
            CPU overhead for compressing batches (especially gzip)
            Small batches (batch.size low / linger.ms low) ‚Üí poor compression efficiency
            High message rate ‚Üí producer threads spend more time compressing than sending
            Content already compressed (JSON with Base64, images) ‚Üí no size gain, only CPU cost
        What to tune:
            Use faster codecs: lz4 or snappy (avoid gzip unless bandwidth is the bottleneck)
            Increase batching: batch.size ‚Üë, linger.ms ‚Üë
            Scale producers or add threads (more producers, not more partitions per producer)
            Disable compression for already-compressed payloads

8.  Messages are unevenly distributed across partitions. What‚Äôs wrong?
    Uneven partition distribution is usually a keying/partitioner issue:
        Same or skewed key (e.g., userId=UNKNOWN, constant key) ‚Üí most records hash to 1‚Äì2 partitions.
        Null key + sticky partitioner ‚Üí producer tends to ‚Äústick‚Äù to one partition per batch; across time it balances, but can look uneven in short windows.
        Custom partitioner bug ‚Üí always returns same partition / bad modulo logic.
        Key has hotspots (few ‚Äúheavy‚Äù users/tenants) ‚Üí natural skew even with correct hashing.
        Too few partitions vs traffic pattern ‚Üí one partition becomes a hotspot.
    Fixes:
        Choose a high-cardinality key (or add a sharding suffix: key = userId + ":" + (hash(orderId)%N)).
        If ordering not required, consider no key (round-robin-ish over time) or a better key.
        Remove/fix the custom partitioner.
        Add partitions only if you can handle rebalancing and key remapping consequences.

9.  One producer floods Kafka and impacts all other services. How do you isolate it?
    Isolate the ‚Äúnoisy‚Äù producer by putting hard limits around it at multiple layers:
    - Separate topic / cluster (best isolation)
        Put that workload on its own topics (and ideally its own cluster if it‚Äôs truly abusive).
    - Quota it on the broker
        Use Kafka client quotas (per client.id / user / IP) to cap produce bandwidth/requests so it can‚Äôt starve others.
    - Dedicated resources
        Separate brokers for that workload (broker rack/tagging, separate cluster)
        Dedicated partitions / replication factor tuned for it
    - Backpressure at the producer
        Lower max.in.flight, limit concurrency, set sane buffer.memory + max.block.ms, rate-limit in app.
    - Multi-tenant hygiene
        Use separate service accounts / credentials and ACLs, so quotas and monitoring can target that one producer cleanly.

10. Producer sends data faster than consumers can handle. How do you handle backpressure?
    Handle backpressure by slowing producers or absorbing load safely:
        - Let Kafka buffer, but cap it
            Consumers scale out (more instances) and/or scale up partitions; add disk/IO on brokers.
        - Rate-limit at producer
            Token bucket / QPS limit, or block on send() by keeping buffer.memory bounded + low max.block.ms 
            (forces the app to slow down instead of OOM).
        - Shed or degrade non-critical traffic
            Drop/skip low-priority events, sample logs/metrics events, or route to a ‚Äúdead letter / overflow‚Äù topic.
        - Use retry + DLQ with limits
            Don‚Äôt retry forever; cap attempts and push failures to DLQ.
        - Consumer-side tuning
            Increase parallelism (more consumers/partitions), optimize processing, and tune max.poll.records / batching.

‚öñÔ∏è Offset & Delivery Semantics Scenarios

1. Offsets are committed but database records are missing. How did this happen?

2. You need exactly-once processing but still see duplicates. Why?

3. Business asks to replay last 3 days of data without breaking systems. How do you do it?

4. A consumer accidentally resets offsets to earliest in production. What do you do next?

5. Duplicate messages corrupt downstream systems. How do you redesign?

6. Offset commits fail intermittently. What are the consequences?

7. How do you safely pause consumption during incidents?

8. A rollback requires reprocessing messages already consumed. How do you handle it?

9. Messages arrive out of order. Why?

10. Consumers reprocess messages after scaling down. Is this expected?

üß± Topic & Partition Design Scenarios

1. Topic was created with too few partitions and now can‚Äôt keep up. What are your options?

2. One partition becomes a hotspot. How do you fix it without downtime?

3. Increasing partitions breaks message ordering. Why?

4. Too many topics exist and Kafka becomes hard to manage. How do you fix governance?

5. A topic grows faster than expected and disk fills up. What do you do?

6. Retention cleanup causes performance degradation. Why?

7. Compacted topic loses data unexpectedly. How?

8. Partition reassignment causes lag spikes. Why?

9. Multi-tenant topics cause noisy-neighbor issues. How do you isolate?

10. What happens if partition leaders move frequently?

üß¨ Schema & Data Evolution Scenarios

1. Producer deploys a schema change and consumers crash. What went wrong?

2. Old consumers break after a new field is added. Why?

3. Schema Registry is down ‚Äî what happens?

4. Different teams evolve schemas independently. How do you prevent outages?

5. A backward-compatible change still causes issues. Why?

6. How do you safely remove a field from events?

7. A schema change corrupts downstream data. How do you recover?

8. Schema compatibility rules are bypassed in production. What‚Äôs the impact?

9. Replay of old events fails due to schema mismatch. Why?

10. How do you test schema changes before production?

üß® Failure & Reliability Scenarios

Kafka broker goes down during peak traffic. What happens?

ISR shrinks frequently. What does that indicate?

Disk usage grows faster than expected. Why?

Network latency spikes affect Kafka cluster. What breaks first?

One broker is much slower than others. Why?

Zookeeper instability causes Kafka issues. How?

Kafka cluster survives but clients start failing. Why?

GC pauses on brokers cause lag. How do you detect?

Messages pile up but brokers look healthy. What do you check?

What failure modes can cause silent data loss?

‚öôÔ∏è Performance & Tuning Scenarios

Throughput suddenly drops by 50% after a config change. How do you debug?

Consumer fetch requests are slow. Why?

High CPU on brokers but low network usage. Why?

Compression improves throughput but increases latency. Why?

Kafka performs well in test but poorly in prod. Why?

Increasing batch size makes latency worse. Why?

Brokers run out of file handles. What caused it?

GC tuning improves throughput but causes latency spikes. Why?

How do you identify the true bottleneck ‚Äî producer, broker, or consumer?

What tuning mistakes commonly cause outages?

üåç Kafka in Distributed Systems Scenarios

A service publishes events but downstream state is inconsistent. Why?

Kafka is used as a request-response system and starts failing. Why?

A single event triggers a cascade of failures. How do you prevent this?

Event-driven workflow partially completes and gets stuck. Why?

Multiple services consume the same topic but interpret data differently. How do you fix this?

Replay causes downstream systems to double charge customers. What went wrong?

Event ordering across services is broken. Why?

Kafka becomes a shared blast radius. How do you redesign?

One bad consumer slows the entire ecosystem. How?

What Kafka anti-patterns have you seen in microservices?

üîê Security & Compliance Scenarios

Unauthorized consumer reads sensitive data. How did this happen?

ACL misconfiguration blocks production traffic. How do you recover?

Certificates expire and Kafka clients start failing. What breaks?

A team bypasses security using shared credentials. How do you prevent this?

Audit team asks who accessed which topics. How do you answer?

Kafka credentials leak. What‚Äôs your immediate action?

TLS handshake failures happen intermittently. Why?

IAM-based auth works in one region but not another. Why?

Security changes break older consumers. Why?

How do you rotate secrets without downtime?

üß† Senior / Architect-Level Scenarios

Kafka is used everywhere and becomes hard to govern. What do you change?

Business wants Kafka for everything. When do you say no?

Kafka cluster cost keeps increasing. How do you optimize?

One team‚Äôs design causes repeated Kafka incidents. How do you fix culturally and technically?

You need cross-region disaster recovery for Kafka. How do you design it?

A migration from MQ to Kafka causes message loss. Why?

You inherit a badly designed Kafka platform. Where do you start?

What Kafka decision caused the biggest outage you‚Äôve seen?

What would you redesign if starting Kafka from scratch?

What Kafka failure changed how you design systems?