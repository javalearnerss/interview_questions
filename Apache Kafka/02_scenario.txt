üî• Kafka Consumer Scenarios

1.  Consumer lag keeps increasing but CPU and memory look normal. How do you debug this?
    When lag increases but CPU/memory look normal, assume the consumer is not actually making progress
    (stuck on I/O, blocked, rebalancing, or reading the wrong partitions). Debug in this order:
        1) Confirm where lag is coming from
            Check per-partition lag (is it one partition/hot key or all partitions?)
            Compare produce rate vs consume rate for the topic

        2) Check for rebalances (most common ‚Äúlooks normal‚Äù cause)
            Look for logs like: revoking partitions / re-joining group
            If rebalances are frequent, investigate:
            long processing delaying poll()
            max.poll.interval.ms too low
            GC pauses / network stalls causing missed heartbeats

        3) Verify the consumer is polling + committing
            Is poll() being called continuously?
            Are offsets actually advancing in the group?
            If offsets aren‚Äôt moving: you‚Äôre stuck in processing or blocked downstream

        4) Look for downstream bottlenecks (CPU can still be normal)
            Common hidden blockers:
            DB/API calls slow or throttled
            Connection pool exhausted
            Synchronous logging / remote calls
            Lock contention (threads waiting, not using CPU)

        5) Check partition assignment vs capacity
            Too few consumers vs partitions ‚Üí some consumers handle many partitions
            One ‚Äúhot partition‚Äù (skewed key) ‚Üí only one consumer falls behind

        6) Check fetch/processing settings
            max.poll.records too high ‚Üí long processing per poll (lag grows)
            fetch.min.bytes / fetch.max.wait.ms too aggressive ‚Üí slow fetch cadence
            Very small batches ‚Üí high overhead (but CPU may still look ‚Äúfine‚Äù)

        7) Look for poison messages / infinite retry
            Same offset repeatedly failing (exceptions + retries)
            Partition appears ‚Äústuck‚Äù on one record ‚Üí use DLQ / skip strategy

2.  After a deployment, consumers stop processing messages but offsets keep committing. What went wrong?
    This usually means the consumer is committing offsets without actually processing records.
    What likely went wrong:
        Auto-commit enabled ‚Üí offsets commit on a timer even if processing logic is broken
        Manual commit placed too early (before processing code)
        Exception swallowed after poll() ‚Üí code skips processing but still commits
        Processing thread not running after deploy (misconfigured thread pool / feature flag)
        Wrong deserializer / schema change ‚Üí records fail silently but commits continue

3.  Kafka consumers restart frequently during peak traffic. Why?
    Kafka consumers restarting frequently during peak traffic is usually due to missed heartbeats or 
    timeouts under load.
    Common reasons:
        Processing takes too long, so poll() isn‚Äôt called within max.poll.interval.ms
        GC pauses / memory pressure during high load cause heartbeat delays
        Downstream calls (DB/API) slow down and block consumer threads
        Too large batches (max.poll.records) increase processing time per poll
        Rebalance storms triggered by slow or unstable consumers

4.  One consumer instance is much slower than others in the same group. How do you investigate?
    If one consumer is much slower than others in the same group, I‚Äôd investigate in this order:
    Check partition assignment ‚Üí see if that consumer owns a hot partition (skewed key, uneven traffic)
    Compare per-partition lag ‚Üí confirm whether slowness is data-related or instance-specific
    Inspect processing path ‚Üí slow DB/API calls, locks, or retries on that instance
    Check GC / pauses ‚Üí GC logs, heap pressure, stop-the-world pauses
    Verify configs & version ‚Üí same code, same configs, same resources as others

5.  Consumers process messages twice after a restart. Is this expected? How do you prevent it?
    Yes ‚Äî this is expected in Kafka under the default at-least-once delivery model.
    Why it happens:
        The consumer processes messages
        It crashes or restarts before committing offsets
        On restart, Kafka resumes from the last committed offset
        Those messages are processed again
        So duplicates after restart are normal unless you design for them.
    How to prevent or handle it:
        Use manual commits after successful processing
        Make consumers idempotent (dedup by eventId, UPSERTs, unique keys)
        For Kafka‚ÜíKafka pipelines, use transactions / exactly-once semantics
        Avoid committing offsets before downstream writes

6.  A consumer crashes on a specific message and keeps reprocessing it forever. What do you do?
    This is a classic poison message scenario.

    What‚Äôs happening:
    The consumer fails on the same record, restarts, reads it again (offset not committed), 
    and crashes repeatedly.

    What to do:
        Identify the bad record (log topic/partition/offset)
        Skip or isolate it:
            Send it to a DLQ (Dead Letter Queue)
            Or manually seek past the offset after logging
        Add bounded retries (retry N times, then give up)
        Fix the root cause (bad data, schema mismatch, deserialization issue)

7.  Rebalancing happens too frequently and throughput collapses. What causes this?
    Frequent rebalancing with throughput collapse is usually caused by consumers 
    repeatedly leaving and rejoining the group.
    Common causes:
        Long processing time ‚Üí poll() not called within max.poll.interval.ms
        GC pauses / JVM stalls under load ‚Üí missed heartbeats
        Blocking downstream calls (DB/API) in the poll thread
        Too many consumer restarts (crash loops, bad liveness probes)
        Over-aggressive configs (session.timeout.ms, heartbeat.interval.ms)
        Eager rebalancing strategy instead of cooperative

8.  Consumers fall behind only at night. What could explain this?
    If consumers fall behind only at night, it‚Äôs usually due to environmental or workload 
    changes, not Kafka itself.
    Common explanations:
        Batch jobs / ETL / backups at night slowing DBs or APIs used by consumers
        Traffic spike (end-of-day processing, reports, settlements) increasing produce rate
        Fewer consumer instances (autoscaling scales down at night)
        Resource contention (shared infra, noisy neighbors)
        Network throttling or maintenance windows
        GC pressure due to larger messages or batch sizes at night
        
9.  One partition is always lagging while others are fine. Why?
    This almost always points to a hot partition problem.
    Why it happens:
        Skewed keys ‚Üí many messages use the same key, all routed to one partition
        Uneven traffic pattern ‚Üí one business entity (user/order/account) dominates
        Slow processing for that partition‚Äôs data (heavier DB calls, retries)
        Single-threaded consumption per partition ‚Üí it can‚Äôt be parallelized further

10. After scaling consumers, throughput doesn‚Äôt improve. Why?
    This usually means you‚Äôve hit a parallelism or bottleneck limit.
    Common reasons:
        Partitions ‚â§ consumers ‚Üí extra consumers sit idle (no more parallelism)
        Hot partition ‚Üí one partition dominates traffic; scaling consumers won‚Äôt help
        Downstream bottleneck ‚Üí DB/API is the real limiter
        Single-threaded processing per partition ‚Üí can‚Äôt parallelize within a partition
        Broker or network limits ‚Üí fetch/produce can‚Äôt go faster


üöÄ Kafka Producer Scenarios

1.  Producer logs show ‚Äúmessage sent successfully‚Äù but consumers don‚Äôt receive anything. How do you debug?
    Debug this like a checklist ‚Äî most ‚Äúsent but not consumed‚Äù issues are topic/offset/group/config problems, 
    not Kafka ‚Äúlosing‚Äù messages.
    1) Confirm the producer really wrote to the right place
    Verify topic name (env prefix, typo, wrong cluster/region)
    Check if records exist with: kafka-console-consumer --from-beginning (no group)

    2) Check consumer group offsets (most common)
    Consumer might be reading but from a different point:
    wrong group.id
    auto.offset.reset=latest + new group (skips old messages)
    offsets already at end
    Use kafka-consumer-groups --describe to see current offsets vs log end

    3) Check subscription & partition assignment
    Ensure consumer is subscribed to the same topic
    Confirm it owns partitions (no rebalancing loop)

    4) Check filtering / deserialization
    Message might be fetched but dropped:
    deserializer/schema mismatch
    app-level filtering (key/type mismatch)
    exceptions swallowed

    5) Check security / connectivity (if applicable)
    ACLs allow producer but not consumer
    SASL/SSL misconfig on consumer side

2.  Producer latency spikes suddenly with no code change. What could cause this?
    - Sudden Kafka producer latency spikes with no code change are usually caused by infrastructure, 
        broker, or config-dependent runtime conditions:
    - Broker load / disk I/O slow (log flush, compaction, high IO wait) ‚Üí slower acks
    - ISR shrink / replica lag with acks=all ‚Üí producer waits longer
    - Leader elections / partition reassignments ‚Üí transient errors + retries
    - Network latency / packet loss / throttling between producer and brokers
    - Request queue saturation on brokers ‚Üí delayed processing/acks
    - GC pauses / CPU steal on producer JVM host ‚Üí delayed send thread
    - Increase in message size or throughput (same code, different traffic) ‚Üí batching/queueing delays
    - Batching effects (linger.ms, batch.size) ‚Üí queues fill, flush timing changes
    - Quota limits / throttling (broker/client quotas) ‚Üí forced delays

3.  Messages are duplicated after enabling retries. Why?
    Messages get duplicated after enabling retries because the producer may resend a message when it 
    doesn‚Äôt receive an acknowledgment‚Äîeven though the broker already wrote it.
     - Ack lost / leader change ‚Üí producer retries
     - Broker writes the same record again ‚Üí duplicate messages
    Fix (one-liner):
     - Enable idempotence: enable.idempotence=true
     ‚Üí broker deduplicates retries

4.  Producers start timing out intermittently under load. What do you tune?
    Intermittent producer timeouts under load ‚Äî tune these (in order):
        Increase send capacity
            batch.size ‚Üë, linger.ms ‚Üë (better batching)
            buffer.memory ‚Üë (avoid local buffer exhaustion)
            Monitor max.block.ms / bufferpool-wait (timeouts often come from waiting for buffer)

        Reduce request pressure
            compression.type = lz4 or snappy
            max.in.flight.requests.per.connection (keep reasonable; with idempotence ‚â§ 5)

        Handle broker/ACK latency
            acks=all + slow replicas can cause timeouts ‚Üí check ISR, disk/network
            request.timeout.ms ‚Üë (if brokers are slow but healthy)
            delivery.timeout.ms ‚Üë (upper bound for retries + waiting)

        Retry/backoff correctly
            retries high (or default), retry.backoff.ms ‚Üë slightly to avoid retry storms

        Rule of thumb line:
        If timeouts happen only under load, it‚Äôs usually buffer pressure (client-side) or broker 
        ack latency (server-side)‚Äîtune accordingly.

5.  A producer crash causes message loss. How is that possible?
    A producer crash can still cause message loss when the message was never durably stored or 
    the app thought it sent but didn‚Äôt verify:
        - Async send without checking: app doesn‚Äôt get()/callback, process crashes before send completes.
        - Buffered in client: record is only in producer memory (buffer.memory/batch) and not yet sent.
        - acks=0 or acks=1 + leader dies: ack returned before replicas copied ‚Üí leader crash can lose it.
        - Retries disabled / low timeouts: transient error + crash before resend.
        - linger.ms/batching window: message waiting to batch when crash happens.
        - Not flushing on shutdown: no flush()/close() ‚Üí in-flight records dropped.

6.  Large messages cause producer OOM errors. How do you fix it?
    Large messages can blow up producer memory because they sit in client buffers, batches, 
    and sometimes compressed copies.
    Fixes (practical + interview-ready):
        - Reduce message size: send only needed fields, compress payload upstream, use efficient 
        formats (Avro/Protobuf), avoid huge JSON.
        - Store payload externally: put large blobs in S3/DB, send only a key + URL/id in Kafka.
        - Raise Kafka size limits (only if you must):
            Producer: max.request.size ‚Üë
            Broker: message.max.bytes ‚Üë (and topic-level if used)
            Consumer: fetch.max.bytes, max.partition.fetch.bytes ‚Üë (otherwise consumers break)
        - Prevent producer memory pressure:
            buffer.memory ‚Üë (more room)
            Reduce batching for big records: batch.size ‚Üì, linger.ms ‚Üì
            compression.type = lz4/snappy (often helps, but doesn‚Äôt fix truly huge records)

    Rule of thumb:
    Kafka is best for events, not big blobs‚Äîuse Kafka for references, not raw large payloads.

7.  Producer throughput drops after enabling compression. Why?
    Producer throughput can drop after enabling compression because compression is CPU-intensive 
    and shifts the bottleneck from network to the producer CPU.
        Why it happens:
            CPU overhead for compressing batches (especially gzip)
            Small batches (batch.size low / linger.ms low) ‚Üí poor compression efficiency
            High message rate ‚Üí producer threads spend more time compressing than sending
            Content already compressed (JSON with Base64, images) ‚Üí no size gain, only CPU cost
        What to tune:
            Use faster codecs: lz4 or snappy (avoid gzip unless bandwidth is the bottleneck)
            Increase batching: batch.size ‚Üë, linger.ms ‚Üë
            Scale producers or add threads (more producers, not more partitions per producer)
            Disable compression for already-compressed payloads

8.  Messages are unevenly distributed across partitions. What‚Äôs wrong?
    Uneven partition distribution is usually a keying/partitioner issue:
        Same or skewed key (e.g., userId=UNKNOWN, constant key) ‚Üí most records hash to 1‚Äì2 partitions.
        Null key + sticky partitioner ‚Üí producer tends to ‚Äústick‚Äù to one partition per batch; across time it balances, but can look uneven in short windows.
        Custom partitioner bug ‚Üí always returns same partition / bad modulo logic.
        Key has hotspots (few ‚Äúheavy‚Äù users/tenants) ‚Üí natural skew even with correct hashing.
        Too few partitions vs traffic pattern ‚Üí one partition becomes a hotspot.
    Fixes:
        Choose a high-cardinality key (or add a sharding suffix: key = userId + ":" + (hash(orderId)%N)).
        If ordering not required, consider no key (round-robin-ish over time) or a better key.
        Remove/fix the custom partitioner.
        Add partitions only if you can handle rebalancing and key remapping consequences.

9.  One producer floods Kafka and impacts all other services. How do you isolate it?
    Isolate the ‚Äúnoisy‚Äù producer by putting hard limits around it at multiple layers:
    - Separate topic / cluster (best isolation)
        Put that workload on its own topics (and ideally its own cluster if it‚Äôs truly abusive).
    - Quota it on the broker
        Use Kafka client quotas (per client.id / user / IP) to cap produce bandwidth/requests so it can‚Äôt starve others.
    - Dedicated resources
        Separate brokers for that workload (broker rack/tagging, separate cluster)
        Dedicated partitions / replication factor tuned for it
    - Backpressure at the producer
        Lower max.in.flight, limit concurrency, set sane buffer.memory + max.block.ms, rate-limit in app.
    - Multi-tenant hygiene
        Use separate service accounts / credentials and ACLs, so quotas and monitoring can target that one producer cleanly.

10. Producer sends data faster than consumers can handle. How do you handle backpressure?
    Handle backpressure by slowing producers or absorbing load safely:
        - Let Kafka buffer, but cap it
            Consumers scale out (more instances) and/or scale up partitions; add disk/IO on brokers.
        - Rate-limit at producer
            Token bucket / QPS limit, or block on send() by keeping buffer.memory bounded + low max.block.ms 
            (forces the app to slow down instead of OOM).
        - Shed or degrade non-critical traffic
            Drop/skip low-priority events, sample logs/metrics events, or route to a ‚Äúdead letter / overflow‚Äù topic.
        - Use retry + DLQ with limits
            Don‚Äôt retry forever; cap attempts and push failures to DLQ.
        - Consumer-side tuning
            Increase parallelism (more consumers/partitions), optimize processing, and tune max.poll.records / batching.

‚öñÔ∏è Offset & Delivery Semantics Scenarios

1. Offsets are committed but database records are missing. How did this happen?
    The consumer can commit the offset even though the DB insert didn‚Äôt actually make it.
    Common real causes:
    - Committed before processing finishes
        Auto-commit enabled, or manual commit placed too early
        ‚Üí crash after commit, before DB write.
    - DB write failed but you still committed
        Exception swallowed / wrong try-catch
        You commit in finally or always-commit logic.
    - Async DB write
        You enqueue an async DB call, commit offset, then app crashes / async call fails.
    - Transaction rollback
        You committed offset, but DB transaction later rolled back (or connection dropped).
    - Rebalance during processing
        Consumer lost partitions, but your code still committed offsets (or commit happened for records not 
        fully processed).

2. You need exactly-once processing but still see duplicates. Why?
    -You‚Äôre only ‚Äúexactly-once‚Äù inside Kafka, not end-to-end
        Kafka EOS guarantees apply to Kafka‚ÜíKafka with transactions. If you write to a DB/HTTP/Elastic, 
        duplicates can still happen unless the sink is idempotent.
    - Consumer retries / crash after side-effect
        App writes to DB, then crashes before committing offset ‚Üí on restart it reprocesses ‚Üí duplicate DB rows/events.
    - Not actually using transactions correctly
        Missing/unstable transactional.id
         Not calling sendOffsetsToTransaction() + commitTransaction() in the right place (Kafka Streams/Exactly-once 
         pattern)
    - Consumers reading uncommitted data
        isolation.level not set to read_committed ‚Üí you may read aborted transactional writes and see 
        ‚Äúduplicates/weirdness‚Äù.
    - Multiple instances producing the same logical event
        Two app instances emit the same business event (at-least-once upstream, retry at API gateway, etc.). 
        Kafka EOS doesn‚Äôt dedupe different producers.
    - No idempotency at the sink
        If your DB insert isn‚Äôt protected by a unique key / upsert, any reprocessing becomes a duplicate.

3. Business asks to replay last 3 days of data without breaking systems. How do you do it?
    Replay into a new consumer group
        Create group.id=replay-2026-01-27 so existing consumers aren‚Äôt affected.

    Limit the window to last 3 days
        Seek by timestamp: use offsetsForTimes(now-3days) (or kafka-consumer-groups --reset-offsets 
        --by-duration P3D --to-earliest/--to-latest as needed).

    Throttle + control blast radius
        Rate-limit the replay consumer (sleep/token bucket), cap concurrency, and keep batch sizes small.

    Make processing idempotent
        Ensure downstream writes are safe: upsert/unique keys, dedupe by eventId, or exactly-once patterns.

    Isolate outputs
        Optionally write replay results to separate topics / DB tables first, then merge.

    Monitor + rollback plan
        Track lag, error rate, and downstream load; be able to pause/stop quickly.

4. A consumer accidentally resets offsets to earliest in production. What do you do next?
    1. Stop / pause the consumer group
        Scale deployment to 0 or pause listeners so it stops re-reading.

    2. Assess blast radius
        Which group.id and topics/partitions?
        How far did it rewind (lag jump)?
        Any downstream impact (DB writes, APIs, emails)?

    3. Protect downstream from duplicates
        Enable/verify idempotency (unique keys/upserts/dedupe by eventId).
        Temporarily disable side effects (emails/notifications) or route to DLQ.

    4. Restore offsets to the correct point
        If you have a known ‚Äúgood‚Äù checkpoint: set offsets back to that timestamp/offset.
        Use kafka-consumer-groups --reset-offsets with:
            --to-datetime <timestamp> or --by-duration or --to-offset
            --execute (after a dry-run)

    5. Controlled restart
        Restart with throttling (lower concurrency / max.poll.records) and monitor.

    6. Post-incident prevention
        Restrict offset reset permissions (ACLs), separate admin creds, add guardrails/runbooks, 
        alerts on sudden lag drops/rewinds.

5. Duplicate messages corrupt downstream systems. How do you redesign?
    - Make consumers idempotent (primary fix)
        Include eventId (or (topic,partition,offset)) in every message
        Downstream writes use unique constraint + upsert / ‚Äúinsert if not exists‚Äù
        Track processed IDs in a dedupe table/cache with TTL if needed

    - Transactional boundaries
        If writing to DB: DB transaction = (apply change + store eventId/offset) atomically
        Use Outbox / Inbox pattern for DB‚ÜîKafka consistency

    - Handle ordering + concurrency
        Use keys so related events go to same partition
        Ensure per-key processing is serialized if required

    - Producer-side safety
        Enable enable.idempotence=true
        Use transactions if producing to multiple topics or doing read‚Üíprocess‚Üíwrite

    - Operational guardrails
        DLQ for poison messages, retry with backoff, and metrics/alerts on dedupe rate

6. Offset commits fail intermittently. What are the consequences?
    - Duplicates / reprocessing: after restart or rebalance, consumer may read old offsets again ‚Üí 
        same messages processed again (at-least-once).

    - Higher lag & slower recovery: consumer keeps working but can‚Äôt ‚Äúadvance‚Äù committed offsets ‚Üí 
        lag looks worse, catch-up after restart takes longer.

    - Possible downstream side-effects: if processing isn‚Äôt idempotent, reprocessing can create 
        duplicate DB rows / repeated emails.

    - Rebalance instability: repeated commit failures can coincide with coordinator/network issues ‚Üí 
        more rebalances, more repeat work.

    - Rare case: if your app treats commit failure as fatal and stops, you get a processing halt 
        (but still no data loss in Kafka).

7. How do you safely pause consumption during incidents?
    Stop the consumer group (hard stop)
        Scale the consumer deployment to 0 / stop the service. No polling, no commits.

    Pause partitions (soft stop, keep membership)
        consumer.pause(assignedPartitions) / in Spring Kafka use pause the listener container. 
        Keeps the consumer alive but stops fetching new records.

    Backpressure throttling (degrade mode)
        Reduce concurrency, lower max.poll.records, add rate-limit/sleeps so downstream isn‚Äôt overwhelmed.

    Protect downstream
        Temporarily disable side effects (emails/external calls) or route to DLQ/holding topic.

8. A rollback requires reprocessing messages already consumed. How do you handle it?
    Create a dedicated ‚Äúrollback replay‚Äù consumer group (don‚Äôt disturb prod group).
    
    Reset that group‚Äôs offsets back to the rollback point:
        by timestamp (best) or exact offsets.

    Reprocess with idempotency:
        use eventId + DB upsert/unique constraint (or a dedupe table) so replay won‚Äôt corrupt data.

    Control blast radius:
        throttle concurrency / max.poll.records, and optionally write results to a staging topic/table first.

    Resume prod safely:
        once data is corrected, switch traffic/consumers back and monitor.

9. Messages arrive out of order. Why?
    Different partitions: same ‚Äúbusiness entity‚Äù events go to different partitions (wrong/nullable key) ‚Üí 
        order not guaranteed.
    Multiple producers: two producers write related events ‚Üí Kafka can‚Äôt enforce cross-producer ordering.
    Retries without idempotence: retries can reinsert and appear out of order (especially with max.in.flight > 1).
    Consumer-side parallelism: processing records concurrently (threads/reactive) finishes in a different 
        order than polled.
    Rebalances / restarts: reprocessing + async processing can make observed order look wrong.

10. Consumers reprocess messages after scaling down. Is this expected?
    Why it happens:
        When you scale down, consumers leave the group.
        Kafka triggers a rebalance.
        Partitions move to remaining consumers.
        New consumers resume from the last committed offset, not the last processed record.
        If some records were processed but offsets weren‚Äôt committed yet ‚Üí they get reprocessed.

    Common reasons it shows up more during scale-down:
        In-flight messages when the consumer shuts down
        Async processing with late commits
        Auto-commit interval not reached yet

üß± Topic & Partition Design Scenarios

1. Topic was created with too few partitions and now can‚Äôt keep up. What are your options?
    Increase partitions (most common)
        kafka-topics --alter --topic X --partitions N
        Pros: more parallelism (more consumers/throughput)
        Cons: order per key changes, key‚Üípartition mapping changes, can increase rebalancing

    Scale consumers + tune processing
        Works only if you already have enough partitions (max parallelism = partitions)
        Optimize handler, batch processing, async IO, max.poll.records, etc.

    Split the topic
        Create X.v2 (or shard topics like X-0..X-7) and route producers by key/tenant
        Pros: clean sharding + better isolation
        Cons: app changes, migration complexity

    Add broker capacity
        More brokers / faster disks / better network if brokers are saturated

    Reduce data volume
        Compression, smaller payloads, drop non-critical events, sampling

2. One partition becomes a hotspot. How do you fix it without downtime?
    Fix a hotspot without downtime by reducing skew and moving load while keeping consumers running:

    Confirm it‚Äôs key skew
        Check if most records share the same/low-cardinality key (or null key).

    Add partitions + rolling update (no downtime)
        Increase partitions for the topic. Consumers will rebalance, but service stays up.

    Change partitioning to spread the hot key
        Add a shard suffix: key = userId + ":" + (hash(orderId) % N)
        Or custom partitioner that splits only hot keys
        Do this via a rolling deploy of producers.

    Split traffic gradually
        Create a new topic topic.v2 with more partitions and dual-write (or route a % of keys) ‚Üí then cut over consumers.

    Short-term mitigations
        Throttle that producer, compress, batch more, or scale brokers if IO-bound.

3. Increasing partitions breaks message ordering. Why?
    Partition is chosen by: hash(key) % numPartitions
    When numPartitions increases, the modulo result changes
    So the same key can start landing on a different partition
    Then events for that key are split across partitions ‚Üí no global ordering for that key anymore

4. Too many topics exist and Kafka becomes hard to manage. How do you fix governance?

5. A topic grows faster than expected and disk fills up. What do you do?
    Immediate containment
        Stop/reduce the producer (feature flag / rate-limit) to stop the bleed.
        Add disk/capacity fast: expand volume / add brokers (if you can quickly).
        Lower retention temporarily:
            reduce retention.ms / retention.bytes
            if safe, use cleanup.policy=delete (not compact-only)
        Run preferred leader / rebalancing only if needed (avoid making it worse during crisis).

    Find the cause
        Which producer/app is spiking? (client.id, ACL user, metrics)
        Is it larger messages, higher rate, or retention misconfig?
        Any stuck consumers causing retries/dup publishes upstream?

6. Retention cleanup causes performance degradation. Why?
    Segment deletions trigger lots of disk IO (delete + metadata updates).
    Log compaction (if enabled) scans and rewrites segments ‚Üí very IO/CPU intensive.
    Frequent cleanup (small segment.ms / aggressive retention.ms) runs too often.
    Slow disks (HDD / saturated SSD) amplify the impact.
    Large partitions mean bigger segments to scan/delete.

7. Compacted topic loses data unexpectedly. How?
    Only the latest value per key is kept
        Older records with the same key are deleted during compaction.

    Tombstones (null values)
        Producing key + null value marks the key for deletion ‚Üí after delete.retention.ms, the key disappears completely.

    Misunderstood retention vs compaction
        If cleanup.policy=compact,delete, old segments can still be deleted by time/size retention.

    Wrong / missing keys
        Records with null keys are not compacted (can be deleted by retention), or changing keys makes data look ‚Äúlost‚Äù.

    Aggressive compaction settings
        Small segment.ms/bytes or frequent cleaner runs ‚Üí faster removal than expected.

8. Partition reassignment causes lag spikes. Why?
    Partition reassignment causes lag spikes because it temporarily steals resources and pauses normal flow.
    Why it happens:
        Data copying: replicas are moved between brokers ‚Üí heavy disk + network IO.
        Throttling kicks in: Kafka slows replication to protect the cluster, delaying fetch/produce.
        Leader movement: leadership changes force consumers to reconnect and catch up.
        Cache cold starts: new leaders don‚Äôt have data in page cache ‚Üí slower reads.
        Extra coordination: metadata updates and ISR changes add overhead.

9. Multi-tenant topics cause noisy-neighbor issues. How do you isolate?

10. What happens if partition leaders move frequently?
    Produce/consume latency spikes (clients must discover new leader + retry).
    More request errors/timeouts (NotLeaderForPartition, LeaderNotAvailable) ‚Üí extra retries.
    Consumer lag increases (fetch pauses during leader change, then catch-up).
    Higher broker load (metadata updates, controller work, replica sync/ISR churn).
    Risk of reduced durability if ISR shrinks often (replicas falling out of sync).

üß¨ Schema & Data Evolution Scenarios

1. Producer deploys a schema change and consumers crash. What went wrong?

2. Old consumers break after a new field is added. Why?

3. Schema Registry is down ‚Äî what happens?

4. Different teams evolve schemas independently. How do you prevent outages?

5. A backward-compatible change still causes issues. Why?

6. How do you safely remove a field from events?

7. A schema change corrupts downstream data. How do you recover?

8. Schema compatibility rules are bypassed in production. What‚Äôs the impact?

9. Replay of old events fails due to schema mismatch. Why?

10. How do you test schema changes before production?

üß® Failure & Reliability Scenarios

Kafka broker goes down during peak traffic. What happens?

ISR shrinks frequently. What does that indicate?

Disk usage grows faster than expected. Why?

Network latency spikes affect Kafka cluster. What breaks first?

One broker is much slower than others. Why?

Zookeeper instability causes Kafka issues. How?

Kafka cluster survives but clients start failing. Why?

GC pauses on brokers cause lag. How do you detect?

Messages pile up but brokers look healthy. What do you check?

What failure modes can cause silent data loss?

‚öôÔ∏è Performance & Tuning Scenarios

Throughput suddenly drops by 50% after a config change. How do you debug?

Consumer fetch requests are slow. Why?

High CPU on brokers but low network usage. Why?

Compression improves throughput but increases latency. Why?

Kafka performs well in test but poorly in prod. Why?

Increasing batch size makes latency worse. Why?

Brokers run out of file handles. What caused it?

GC tuning improves throughput but causes latency spikes. Why?

How do you identify the true bottleneck ‚Äî producer, broker, or consumer?

What tuning mistakes commonly cause outages?

üåç Kafka in Distributed Systems Scenarios

A service publishes events but downstream state is inconsistent. Why?

Kafka is used as a request-response system and starts failing. Why?

A single event triggers a cascade of failures. How do you prevent this?

Event-driven workflow partially completes and gets stuck. Why?

Multiple services consume the same topic but interpret data differently. How do you fix this?

Replay causes downstream systems to double charge customers. What went wrong?

Event ordering across services is broken. Why?

Kafka becomes a shared blast radius. How do you redesign?

One bad consumer slows the entire ecosystem. How?

What Kafka anti-patterns have you seen in microservices?

üîê Security & Compliance Scenarios

Unauthorized consumer reads sensitive data. How did this happen?

ACL misconfiguration blocks production traffic. How do you recover?

Certificates expire and Kafka clients start failing. What breaks?

A team bypasses security using shared credentials. How do you prevent this?

Audit team asks who accessed which topics. How do you answer?

Kafka credentials leak. What‚Äôs your immediate action?

TLS handshake failures happen intermittently. Why?

IAM-based auth works in one region but not another. Why?

Security changes break older consumers. Why?

How do you rotate secrets without downtime?

üß† Senior / Architect-Level Scenarios

Kafka is used everywhere and becomes hard to govern. What do you change?

Business wants Kafka for everything. When do you say no?

Kafka cluster cost keeps increasing. How do you optimize?

One team‚Äôs design causes repeated Kafka incidents. How do you fix culturally and technically?

You need cross-region disaster recovery for Kafka. How do you design it?

A migration from MQ to Kafka causes message loss. Why?

You inherit a badly designed Kafka platform. Where do you start?

What Kafka decision caused the biggest outage you‚Äôve seen?

What would you redesign if starting Kafka from scratch?

What Kafka failure changed how you design systems?