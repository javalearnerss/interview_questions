
üü† Level 3 ‚Äî Senior Developer / Production Ownership
Deployment Strategies
20. Rolling update vs Blue-Green vs Canary ‚Äî when to use?
21. How do you prevent downtime during deployment?
22. What happens to in-flight requests during rollout?

Resource Management
23. What are resource requests vs limits?
24. What happens if Java app exceeds memory limit?
25. What is OOMKilled?

Failure Scenarios
26. Node crashes ‚Äî what happens to your app?
27. Pod restarts ‚Äî what happens to cache/session?
28. How does Kubernetes self-heal?

üî¥ Level 4 ‚Äî Senior+ / Lead Level (Many Companies Ask)
Production Design
29. How would you design Spring Boot microservices in Kubernetes?
30. How would you design Kafka consumers in Kubernetes?
31. How would you design DB connections inside Kubernetes?

***********************************************************************************************************************************
20. Rolling update vs Blue-Green vs Canary ‚Äî when to use?

    Rolling Update : Rolling update is Kubernetes gradually replacing old pods with new pods while keeping the application available ‚Äî but 
    internally it‚Äôs driven by the Deployment controller, ReplicaSets, and pod health checks.

    Step 1: You Trigger a Deployment Update
        Example:
        New Docker image pushed
        You run: kubectl apply -f deployment.yaml
        
        Kubernetes detects: Deployment spec changed (image, env, config, etc.)

    Step 2: Kubernetes Creates a New ReplicaSet
        Important concept:
        Every Deployment manages:
        1. Old ReplicaSet (v1 pods)
        2. New ReplicaSet (v2 pods)

        Example:
                Before:
                    ReplicaSet-v1 ‚Üí 5 pods
    
                After update triggered:
                    ReplicaSet-v1 ‚Üí 5 pods
                    ReplicaSet-v2 ‚Üí 0 pods (new, just created)  

    Step 3: Rolling Strategy Starts Executing
        Based on: maxSurge, maxUnavailable

        Example:
            replicas = 5
            maxSurge = 1
            maxUnavailable = 1

        Rules:
            Can add 1 extra pod
            Can remove 1 old pod at a time

    Step 4: New Pod Creation Starts
        Kubernetes increases new ReplicaSet size:
            ReplicaSet-v2 ‚Üí 1 pod created

        Total pods now: 5 old + 1 new = 6

    Step 5: New Pod Must Become READY
        Kubernetes waits for:
            ‚úî Container started
            ‚úî Readiness probe passed
            ‚úî App responding
            ‚úî Dependencies reachable

        If not ready:
            Rolling update pauses.

    Step 6: Old Pod Termination Starts
        Once new pod is ready:
        Kubernetes reduces old ReplicaSet:
            1. ReplicaSet-v1 ‚Üí 4 pods
            2. ReplicaSet-v2 ‚Üí 1 pod

    Step 7: Load Balancer / Service Updates Automatically
        Kubernetes Service only sends traffic to:
        üëâ READY pods only
        So traffic shifts gradually:
            Old pods ‚Üí less traffic
            New pods ‚Üí more traffic

    Step 8: Process Repeats Until Completion
        Cycle repeats:
        Create new pod ‚Üí Wait ready ‚Üí Delete old pod
            Final state:
                ReplicaSet-v1 ‚Üí 0
                ReplicaSet-v2 ‚Üí 5

    Timeline Table (Simple View)
    Time	    Old Pods	New Pods	Total
     T0	        3	        0	        3
     T1	        3	        1	        4
     T2	        2	        1	        3
     T3	        2	        2	        4
     T4	        1	        2	        3
     T5	        1	        3	        4
     T6	        0	        3	        3

     -------------------------------------------------------------------------------------

    Blue-Green Deployment ‚Äî Interview Ready Points
        Definition
            Blue-Green deployment is a release strategy using two identical production environments.
            One environment is Blue (current live), the other is Green (new version / staging).

        How It Works
            Deploy the new application version to the Green environment.
            Perform testing, validation, and health checks on Green.
            Once verified, switch production traffic from Blue ‚Üí Green (via Load Balancer / DNS).
            Keep Blue running temporarily for quick rollback if issues occur.

    How It Works ‚Äî Step by Step
        Step 1 ‚Äî Blue is Live (Current Production)
            Example:
                Blue Environment ‚Üí Inventory Service v1 ‚Üí Serving Users
                Green Environment ‚Üí Not used yet
        
        Step 2 ‚Äî Deploy New Version to Green
            Deploy:
                Green Environment ‚Üí Inventory Service v2

            But:
                ‚ùå No user traffic yet
                ‚úÖ You can test safely

        Step 3 ‚Äî Test Green Environment
            Teams test:
                1. Smoke tests
                2. DB connectivity
                3. Cache connectivity
                4. Booking flow
                5. Performance

            Green is like shadow production.

        Step 4 ‚Äî Switch Traffic (Critical Moment)
            Load balancer / router switches:
                FROM ‚Üí Blue
                TO ‚Üí Green

            Now:
             All users go to Green (new version)

        Step 5 ‚Äî Keep Blue as Backup (Optional)
            If everything is stable:
                Delete Blue
                OR
                Keep as rollback backup

        Why Blue-Green is Very Safe
        Because:
            ‚úî No mixing old + new versions
            ‚úî Easy rollback (just switch traffic back)
            ‚úî Full production testing possible
            ‚úî No compatibility issues during rollout

    --------------------------------------------------------------------------------------------------------------------
    Canary Deployment
        Canary Deployment means:
            1. Release new version to small % of users first
            2. Monitor system behavior
            3. Gradually increase traffic if safe

            Named after:
            Canary birds used in coal mines ‚Äî early warning system.
        
        Simple Definition
            Instead of:
                1. Replacing all pods (Rolling)
                2. Switching all traffic (Blue-Green)

            Canary sends:
                1. Small traffic ‚Üí New version
                2. Most traffic ‚Üí Old version

        How It Works ‚Äî Step by Step
            Step 1 ‚Äî Deploy New Version Alongside Old
                Example:
                    1. Old Version ‚Üí 95% traffic
                    2. New Version ‚Üí 5% traffic

            Step 2 ‚Äî Monitor Metrics
                Teams watch:
                    1. Error rate
                    2. Latency
                    3. DB load
                    4. Booking failures
                    5. Lock conflicts
                    6. If safe ‚Üí Increase traffic.

            Step 3 ‚Äî Gradually Increase Traffic
                Example rollout: 5% ‚Üí 20% ‚Üí 50% ‚Üí 100%   

            Step 4 ‚Äî Full Rollout
                If everything stable: New version becomes main version.

***********************************************************************************************************************************
21. How do you prevent downtime during deployment?


***********************************************************************************************************************************
22. What happens to in-flight requests during rollout?
    What is an In-Flight Request?
        In-flight requests are requests currently being processed by the application but not yet completed. During deployments or shutdowns,
         we allow in-flight requests to finish before terminating the pod to avoid user errors or data inconsistency.

    In-flight request in a ticket booking application:

        ‚Ä¢ An in-flight request is a booking operation (e.g., user submitting a ticket purchase) that is being processed by 
            the application but has not yet completed (user has not received confirmation).
        ‚Ä¢ Examples:
            o User clicks "Book Ticket" and the server is processing payment and seat allocation.
            o User requests seat availability, and the server is fetching data.

        Deployment impact:

        ‚Ä¢ If the application is redeployed or restarted while processing in-flight requests, those requests may be interrupted, leading to:
            o Lost bookings
            o Payment errors
            o Inconsistent seat allocation

        Handling in-flight requests:

            ‚Ä¢ Use graceful shutdown:
            o When deployment starts, stop accepting new booking requests.
            o Allow current in-flight requests to finish processing before shutting down the server.

        ‚Ä¢ In Spring Boot:
             o Enable graceful shutdown so the application waits for ongoing HTTP requests to complete.

        ‚Ä¢ Use load balancer health checks:
             o Remove instance from load balancer before shutdown, so only in-flight requests are processed.

    During rollout, Kubernetes first creates new pods and only sends traffic after readiness passes. When old pods are terminated, 
    they receive SIGTERM and are marked not ready, so they stop receiving new traffic. However, existing in-flight requests are 
    allowed to finish within terminationGracePeriodSeconds. In Spring Boot, graceful shutdown ensures the server stops accepting 
    new requests but allows active requests to complete, preventing user errors and partial transactions.

***********************************************************************************************************************************
23. What are resource requests vs limits?

    Resource Request:

        ‚Ä¢ The minimum amount of CPU and memory (RAM) guaranteed to a container.
        ‚Ä¢ The scheduler uses requests to decide where to place a pod.
        ‚Ä¢ If a node does not have enough unallocated resources to meet the request, the pod will not be scheduled there.

    Resource Limit:

        ‚Ä¢ The maximum amount of CPU and memory a container can use.
        ‚Ä¢ If the container tries to use more than the limit, it may be throttled (CPU) or killed (memory).

    Purpose:

        ‚Ä¢ Prevents a single container from consuming all resources on a node.
        ‚Ä¢ Ensures fair resource allocation and stability.

    Example (Kubernetes YAML):

    resources:
      requests:
        memory: "512Mi"
        cpu: "500m"
      limits:
        memory: "1Gi"
        cpu: "1"

    ‚Ä¢ Here, the container is guaranteed 512MiB RAM and 0.5 CPU, but cannot use more than 1GiB RAM or 1 CPU.


        Summary Table:

        Term        Description                                   Effect
        Request     Minimum resources reserved for the container  Scheduler uses for pod placement
        Limit       Maximum resources container can use           Enforced at runtime (throttle/kill)

***********************************************************************************************************************************
24. What happens if Java app exceeds memory limit?

    If a Java app exceeds its Kubernetes memory limit, the container is terminated by the Linux OOM killer and Kubernetes marks it as OOMKilled, 
    usually with exit code 137. The container is then restarted, and if it keeps happening, it leads to CrashLoopBackOff. For Java apps, 
    this is common because total JVM memory includes heap, metaspace, and native memory. In production, we tune JVM memory using container-aware 
    settings like MaxRAMPercentage and leave headroom beyond heap.

***********************************************************************************************************************************
25. What is OOMKilled?
    OOMKilled = Out Of Memory Killed

    üëâ It means the container used more memory than its Kubernetes memory limit,
    üëâ So the Linux kernel forcefully terminated it.

    What Triggers OOMKilled
    Example:

    resources:
    limits:
        memory: 1Gi

    If container memory usage ‚Üí 1.2Gi

    Result:
        Linux OOM Killer kills container
        Kubernetes marks pod:

        Reason: OOMKilled
        Exit Code: 137

    What Happens After OOMKilled
        1Ô∏è‚É£ Container is killed immediately (no graceful shutdown)
        2Ô∏è‚É£ In-flight requests fail
        3Ô∏è‚É£ Kubernetes restarts container
        4Ô∏è‚É£ If repeated ‚Üí CrashLoopBackOff

***********************************************************************************************************************************
26. Node crashes ‚Äî what happens to your app?

    1. Pod Disruption:
        All pods (including your Spring Boot app) running on the crashed node become unreachable and 
        stop serving traffic.

    2. Kubernetes Response:
        The Kubernetes control plane detects the node failure (via node heartbeats).
        Pods on the failed node are marked as "Unknown" or "NotReady".
        After a timeout (default ~5 minutes), Kubernetes evicts the pods from the failed node.

    3. Pod Rescheduling:
        Kubernetes automatically schedules replacement pods on healthy nodes, if resources are available.
        The new pods start up and register with the service/load balancer.

    4. Effect on Spring Boot App:
        Any in-flight requests to the crashed node are lost.
        New requests are routed to healthy pods.
        If there are enough replicas, the application remains available (no downtime).
        If only one replica existed, the app is unavailable until the pod restarts on another node.

    5. Best Practices:
        Use multiple replicas for high availability.
        Use readiness/liveness probes for fast failover.
        Externalize session state to avoid user session loss.
    
    Event                | Kubernetes Action                | Spring Boot App Effect

    Node crashes         | Pods on node become unreachable  | Requests to those pods fail

    Node marked NotReady | Pods evicted, rescheduled        | New pods start elsewhere

    Sufficient replicas  | Traffic routed to healthy pods   | Minimal/no downtime

    Single replica       | App unavailable until restart    | Temporary downtime possible

*****************************************************************************************************************************
28. What happens to cache/session when a Pod restarts in Kubernetes?

    When a Pod restarts, anything stored inside the Pod memory or local filesystem is lost, unless the data is stored externally 
    or on persistent storage.

    ‚úÖ Cache behavior
        If cache is inside the Pod (in-memory / local cache like Spring Cache, Ehcache, local Redis instance)
        Cache is cleared on restart
        First requests after restart may be slower (cache warm-up)
        If cache is external (Redis, Memcached, Hazelcast cluster)
        Cache survives Pod restart
        Other Pods continue using the same cache

    ‚úÖ Session behavior
        If sessions stored inside Pod memory (sticky sessions / in-memory HTTP session)
        User sessions are lost on Pod restart
        Users may get logged out or lose cart state
        If sessions stored externally (Redis / DB / distributed session store)
        Sessions remain intact
        Users don‚Äôt notice Pod restart

    ‚úÖ Storage summary
    Storage Type	        After Pod Restart
    Container Memory	    ‚ùå Lost
    emptyDir	            ‚ùå Lost
    Persistent Volume	    ‚úÖ Preserved
    External Cache/DB	    ‚úÖ Preserved


    ‚úÖ Ticket Booking Example (Strong interview closer)
        ‚ÄúIf my booking app stores user session and seat lock data inside the Pod, a restart would lose that data. So in production, 
        I store sessions and cache in Redis so Pod restarts are transparent to users.‚Äù

**************************************************************************************************************
29. How would you design Spring Boot microservices in Kubernetes?
    1) Service boundaries and responsibilities
        API Gateway
            Single entry point for clients (web/mobile).
            Routes to /users, /bookings, /payments, /inventory.
            Handles TLS termination, auth enforcement, rate limiting.

        User Service
            Signup/login/profile.
            Issues/validates JWT (or integrates with Keycloak/OAuth).
            Stores user data in its own DB.

        Booking Service
            Orchestrates booking flow: ‚Äúcreate booking ‚Üí reserve seats ‚Üí initiate payment ‚Üí confirm/cancel‚Äù.
            Owns booking state machine: PENDING ‚Üí CONFIRMED or CANCELLED/EXPIRED.

        Inventory Service
            Owns seat availability and seat locks.
            Prevents overselling using DB constraints or locking strategy.

        Payment Service
            Talks to payment gateway.
            Ensures idempotency: same payment request should not double-charge.
            K8s angle: Each of these is its own Deployment (stateless) with its own Service (DNS-based discovery).

    2) Stateless design (what must NOT live in Pods)
        In Kubernetes, Pods can restart anytime, so don‚Äôt keep critical state in memory.
            User sessions: don‚Äôt store in pod memory. Use JWT (stateless) or store sessions in Redis.
            Seat locks: never keep only in-memory. Store locks in Redis with TTL or DB.
            Booking status: store in Booking DB; never rely on cached state.
        Result: if a Booking/Inventory pod restarts, users don‚Äôt lose their booking/lock.

    3) Data ownership (DB per service)
        User DB: users, roles, login metadata
        Booking DB: bookings, booking_items, status, expiry timestamps
        Inventory DB: events, seats, seat_status, reservations/locks
        Payment DB: payment_intents, provider refs, status

        Why interviewers like it: independent scaling + independent schema changes + fewer cross-service coupling issues.

    4) Communication style (sync + async)
        Synchronous (REST)
            Gateway ‚Üí all services (client calls)
            Booking ‚Üí Inventory (reserve/lock seats)
            Booking ‚Üí Payment (create payment intent)
        Asynchronous (events) for reliability
            Inventory emits SeatsReserved, SeatsReleased
            Payment emits PaymentSucceeded, PaymentFailed
            Booking consumes events to finalize booking

        Why: avoids tight coupling and handles retries safely. If Payment is slow, Booking doesn‚Äôt block forever.

    5) Resilience patterns (critical for booking/payment)
        In Booking ‚Üí Inventory/Payment calls:
            Timeouts (don‚Äôt hang)
            Retries with backoff (only for safe operations)
            Circuit breaker (avoid cascading failures)
            Bulkheads (separate thread pools for payment vs inventory)

        Idempotency
            POST /payments must accept an Idempotency-Key so retries don‚Äôt double-charge.
            POST /bookings can use a client-generated request id to avoid duplicate bookings.

    6) Kubernetes objects (mapped to your app)
        API Gateway
            Deployment + Service + Ingress (or Gateway API)
            Often needs higher CPU and strict autoscaling.
        Booking/Inventory/Payment/User
            Deployment + Service (ClusterIP)
            HPA for scale
            ConfigMap/Secret for config
        External dependencies
            Redis (cache/locks) ‚Üí managed preferred; if in-cluster then StatefulSet + PV
            Kafka/RabbitMQ ‚Üí managed preferred

    7) Health probes (Actuator, per service)
        Use Spring Boot Actuator:
        Liveness: ‚ÄúIs the process alive?‚Äù
            /actuator/health/liveness
            If it fails ‚Üí K8s restarts the container
        Readiness: ‚ÄúCan this pod serve traffic now?‚Äù
            /actuator/health/readiness
            Only route traffic if it‚Äôs Ready

        Examples
            Booking readiness should fail if it can‚Äôt reach Booking DB (otherwise it will accept requests and fail).
            Inventory readiness should consider Inventory DB + Redis (if Redis is used for seat locks).
            Payment readiness should consider Payment DB + payment provider connectivity (or at least DNS/egress).

    8) Config management (dev/stage/prod)
        Same Docker image everywhere.
            ConfigMap: URLs, feature flags, timeouts, log levels
            Secrets: DB passwords, payment gateway keys, JWT signing keys

            Example:
                PAYMENT_PROVIDER_KEY in Secret
                INVENTORY_LOCK_TTL_SECONDS in ConfigMap
        This keeps deployments repeatable and secure.

    9) Deployment strategy (rolling/canary) ‚Äî booking-specific
        Rolling update (default)
            Good for backward-compatible changes (new endpoint, internal refactor).
            Ensure readiness probes so traffic doesn‚Äôt hit half-started pods.
        Canary / Blue-Green
            Use for risky changes:
                inventory lock algorithm change
                payment flow change
                DB schema changes affecting critical paths
        Important rule for booking apps: Maintain backward compatibility between versions during rollout (schema + API).

    10) Scaling strategy (HPA) ‚Äî per service
        API Gateway: scales on CPU/RPS quickly (spikes)
        Inventory: scales with ‚Äúsearch availability / seat hold‚Äù traffic (high during sales)
        Booking: scales with checkout traffic
        Payment: scales but protect provider limits (rate limiting + queue)
        User: moderate scaling unless login spikes
        Also use topology spread so replicas run across nodes/zones (node failure shouldn‚Äôt take down all replicas).

    11) Security (very relevant)
        Gateway validates JWT and forwards identity headers downstream (or services validate too).
        RBAC: services only get the K8s permissions they need (often none).
        NetworkPolicy: only allow:
        Gateway ‚Üí services
        Booking ‚Üí Inventory/Payment
        Services ‚Üí DB/Redis/Kafka
        Secrets never in images or Git.

    12) Observability (how you debug production issues)
        Central logs (stdout ‚Üí ELK/Cloud logging)
        Metrics (Micrometer ‚Üí Prometheus/Grafana)
        booking success rate, payment latency, inventory lock failures
        Tracing (OpenTelemetry)
        trace ID from Gateway ‚Üí Booking ‚Üí Inventory/Payment
        helps debug ‚Äúwhy booking failed‚Äù across services
        Interview line: ‚ÄúOne booking request should be traceable end-to-end with a single traceId.‚Äù

    13) Consistency & preventing oversell (inventory focus)
        Options:
            DB-level constraints (seat row locked / unique constraint per seat per show)
            Redis lock with TTL for temporary holds
            Lock expiry: seats return automatically if payment not completed in X minutes
            Booking service runs a saga:
            Reserve seats ‚Üí Start payment ‚Üí Confirm seats ‚Üí Confirm booking
            On failure ‚Üí release seats

    A strong 60-second version you can speak

    ‚ÄúI split the ticket booking system into Gateway, Booking, Inventory, Payment, and User services. Each runs as a stateless Spring Boot 
    Deployment on Kubernetes, with state stored in per-service databases and Redis for sessions/seat locks. Booking orchestrates the 
    saga‚Äîreserves seats via Inventory, creates a payment intent via Payment, and finalizes on success events. I use Actuator 
    readiness/liveness probes, ConfigMaps/Secrets for environment config, HPA for scaling, and safe rollout strategies like rolling by 
    default and canary for risky payment/inventory changes. Finally, I ensure end-to-end observability with metrics and tracing so a 
    single booking can be tracked across all services.‚Äù

***************************************************************************************************************************************************
30. How would you design Kafka consumers in Kubernetes?

    Kafka Consumer Design in Kubernetes (Theory):

    1. Architecture:
        Each Kafka consumer instance runs inside a container (pod) managed by Kubernetes.
        Consumers are grouped by a consumer group ID; each instance processes a subset of Kafka topic partitions.
        Spring Boot can be used to implement the consumer logic, leveraging the @KafkaListener annotation.

    2. Scaling:
        Kubernetes Deployment manages the number of consumer replicas (pods).
        Increasing replicas allows more parallel processing, but the number of consumers should not exceed the number of topic partitions (otherwise, some consumers will be idle).
        Scaling is achieved by updating the replica count in the Deployment manifest.

    3. Fault Tolerance:
        If a pod fails or a node crashes, Kubernetes automatically restarts the pod on a healthy node.
        Kafka rebalances partition assignments among available consumers in the group.
        This ensures continuous message processing and high availability.

    4. Resource Management:
        Each pod specifies resource requests and limits (CPU, memory) to ensure fair allocation and prevent resource exhaustion.
        Proper sizing prevents OOM kills and ensures stable operation.

    5. Configuration:
        Consumer properties (bootstrap servers, group ID, offset reset, concurrency) are set via environment variables or config files.
        Sensitive data (e.g., credentials) can be managed using Kubernetes Secrets.
    
    6. Health Checks:
        Readiness and liveness probes are configured so Kubernetes only routes traffic to healthy pods and restarts unhealthy ones.
        Spring Boot Actuator endpoints are commonly used for these probes.

    7. Graceful Shutdown:
        When a pod is terminated (e.g., during deployment or scaling), Kubernetes signals the container to shut down.
        The consumer should finish processing in-flight messages and commit offsets before exiting.
        Spring Boot supports graceful shutdown, ensuring no message loss.

    8. Error Handling:
        Consumers should handle deserialization errors, processing failures, and commit errors.
        Failed messages can be sent to a dead-letter topic for later analysis.

    9. Observability:
        Logging, metrics, and tracing are integrated for monitoring consumer health and performance.
        Tools like Prometheus and Grafana can be used for visualization.

    Summary Table:
    Aspect	                    Description
    Architecture	        Consumers run as pods, grouped by consumer group, process topic partitions
    Scaling	                Managed by Deployment replicas, limited by partition count
    Fault Tolerance	        Automatic pod restart, partition reassignment, high availability
    Resource Mgmt	        Requests/limits for CPU/memory, prevents resource exhaustion
    Configuration	        Environment variables, config files, secrets
    Health Checks	        Readiness/liveness probes, Actuator endpoints
    Graceful Shutdown	    Finish in-flight messages, commit offsets, prevent message loss
    Error Handling	        Handle failures, dead-letter topic for problematic messages
    Observability	        Logging, metrics, tracing, monitoring tools

********************************************************************************************************************************
31. How would you design DB connections inside Kubernetes?